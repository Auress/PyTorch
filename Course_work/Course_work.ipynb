{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Курсовая работа#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Автор: Шенк Евгений Станиславович"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "sys.path.insert(0, './yolo-hand-detection')\n",
    "\n",
    "from yolo import YOLO\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры для обучения\n",
    "batch_size = 200\n",
    "epochs = 10\n",
    "max_lr = 0.008\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = tt.Compose([tt.Grayscale(num_output_channels=1), # Картинки чернобелые\n",
    "                         \n",
    "                         tt.CenterCrop(250),\n",
    "                         tt.Resize(100),\n",
    "                               \n",
    "                         # Настройки для расширения датасета\n",
    "                         tt.RandomRotation(45),               # Случайные повороты на 30 градусов\n",
    "                         tt.ToTensor()])                      # Приведение к тензору\n",
    "\n",
    "#test_transforms = tt.Compose([tt.Grayscale(num_output_channels=1), tt.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '01', '02', '03', '04', '05', '06', '07', '08', '09']\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data/leapGestRecog'\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_to_classname = {0:'palm', 1:'l', 2:'fist', 3:'fist_moved', 4:'thumb', 5:'index',\n",
    "                      6:'ok', 7:'palm_moved', 8:'c', 9:'down'}\n",
    "num_classes = len(digit_to_classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None\n",
    "\n",
    "for folder in os.listdir(data_dir):\n",
    "    if dataset is not None:\n",
    "        data_to_add = ImageFolder(data_dir + f'/{folder}', train_transforms)\n",
    "        dataset = ConcatDataset([dataset, data_to_add])\n",
    "    else:\n",
    "        dataset = ImageFolder(data_dir + f'/{folder}', train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2177)\n",
    "num_of_elements = int(len(dataset))\n",
    "train_part = 0.7 # часть датасета для обучения\n",
    "train_dataset, test_dataset = random_split(dataset, [int(train_part * num_of_elements), int((1-train_part) * num_of_elements)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 100])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DeviceDataLoader(train_dataloader, device)\n",
    "test_dataloader = DeviceDataLoader(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pool_2 = nn.MaxPool2d(2)\n",
    "        self.pool_6 = nn.MaxPool2d(6)\n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(3136, 512) #576 - 3136\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool_2(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool_6(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool_6): MaxPool2d(kernel_size=6, stride=6, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (flat): Flatten()\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = to_device(Net(1, num_classes), device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nepoch_losses = []\\n\\nfor epoch in range(epochs):\\n    \\n    time1 = time.time()\\n    running_loss = 0.0\\n    epoch_loss = []\\n    for batch_idx, (data, labels) in enumerate(train_dataloader):\\n        data, labels = Variable(data), Variable(labels)\\n        data = data.to(device)\\n        labels = labels.to(device)\\n        \\n        \\n        optimizer.zero_grad()\\n        \\n        outputs = model(data)\\n        loss = F.cross_entropy(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n        scheduler.step()\\n        \\n        running_loss += loss.item()\\n        epoch_loss.append(loss.item())\\n        if (batch_idx+1) % 100 == 99:\\n            print(f'Train Epoch: {epoch+1}, Loss: {running_loss/10000}')\\n            time2 = time.time()\\n            print(f'Spend time for 10000 images: {time2 - time1} sec')\\n            time1 = time.time()\\n            running_loss = 0.0\\n    print(f'Epoch {epoch+1}, loss: ', np.mean(epoch_loss))\\n    epoch_losses.append(epoch_loss)\\n    \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    time1 = time.time()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "        data, labels = Variable(data), Variable(labels)\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        epoch_loss.append(loss.item())\n",
    "        if (batch_idx+1) % 100 == 99:\n",
    "            print(f'Train Epoch: {epoch+1}, Loss: {running_loss/10000}')\n",
    "            time2 = time.time()\n",
    "            print(f'Spend time for 10000 images: {time2 - time1} sec')\n",
    "            time1 = time.time()\n",
    "            running_loss = 0.0\n",
    "    print(f'Epoch {epoch+1}, loss: ', np.mean(epoch_loss))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "# torch.save(model.state_dict(), './Hand_gesture_recognition_model_100_state_10_epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(1, num_classes).to(device) \n",
    "net.load_state_dict(torch.load('./Hand_gesture_recognition_model_100_state_10_epoch.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eugene\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# Создаем объект для считывания потока с веб-камеры(обычно вебкамера идет под номером 0. иногда 1)\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "yolo = YOLO(\"yolo-hand-detection/models/cross-hands.cfg\", \"yolo-hand-detection/models/cross-hands.weights\",\n",
    "            [\"hand\"], confidence=0.5, threshold=0.3)\n",
    "\n",
    "# Класс детектирования и обработки лица с веб-камеры \n",
    "class FaceDetector(object):\n",
    "\n",
    "    def __init__(self, mtcnn):\n",
    "        self.mtcnn = mtcnn\n",
    "        self.hand_detector = yolo\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.emodel = Net(1, 10).to(self.device)\n",
    "        self.emodel.load_state_dict(torch.load('./Hand_gesture_recognition_model_100_state_10_epoch.pth'))\n",
    "        self.emodel.eval()\n",
    "\n",
    "    # Функция рисования найденных параметров на кадре\n",
    "    def _draw(self, frame, boxes, probs, landmarks):\n",
    "        try:\n",
    "            for box, prob, ld in zip(boxes, probs, landmarks):\n",
    "                # Рисуем обрамляющий прямоугольник лица на кадре\n",
    "                cv2.rectangle(frame,\n",
    "                              (box[0], box[1]),\n",
    "                              (box[2], box[3]),\n",
    "                              (0, 0, 255),\n",
    "                              thickness=2)\n",
    "        except:\n",
    "            pass\n",
    "            #print('Something wrong im draw function!')\n",
    "\n",
    "        return frame\n",
    "    \n",
    " \n",
    "    # Функция для вырезания рук и с кадра\n",
    "    @staticmethod\n",
    "    def crop_hand(frame, boxes, exp=0):\n",
    "        hands = []\n",
    "        for i, box in enumerate(boxes):\n",
    "            hands.append(frame[int(box[1])-exp:int(box[3])+exp, \n",
    "                int(box[0])-exp:int(box[2])+exp])\n",
    "        return hands\n",
    "    \n",
    "    # Словарь жестов\n",
    "    @staticmethod\n",
    "    def digit_to_classname(digit):\n",
    "        digit_to_classname_dict = {0:'palm', 1:'l', 2:'fist', 3:'fist_moved', 4:'thumb', 5:'index',\n",
    "                                  6:'ok', 7:'palm_moved', 8:'c', 9:'down'}\n",
    "        return digit_to_classname_dict[digit]\n",
    "    \n",
    "    # Функция реакции на жест\n",
    "    def gesture_action(self, digit, frame):\n",
    "        if digit is None:\n",
    "            return \"\", frame \n",
    "        \n",
    "        gesture_to_show = self.digit_to_classname(digit)\n",
    "        if digit in [3, 5, 7, 9]:  # Данные положения руки плохо определяются детектором, поэтому только надпись\n",
    "            pass\n",
    "        if digit == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if digit == 1:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        if digit == 2:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        if digit == 4:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        if digit == 6:  \n",
    "            pass  # Вернуть изначальный вариант\n",
    "        if digit == 8:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        return gesture_to_show, frame\n",
    "        \n",
    "    # Функция в которой будет происходить процесс считывания и обработки каждого кадра\n",
    "    def run(self):              \n",
    "        gesture = ''\n",
    "        gesture_to_show = ''\n",
    "        gesture_idx = None\n",
    "        guess_list = np.array([0] * 10)\n",
    "        x, y = [0, 0]\n",
    "        \n",
    "        # Заходим в бесконечный цикл\n",
    "        while True:\n",
    "            # Считываем каждый новый кадр - frame\n",
    "            # ret - логическая переменая. Смысл - считали ли мы кадр с потока или нет\n",
    "           \n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            try:\n",
    "                # детектируем расположение лица на кадре, вероятности на сколько это лицо\n",
    "                # и особенные точки лица\n",
    "                boxes, probs, landmarks = self.mtcnn.detect(frame, landmarks=True)\n",
    "                # Ищем лицо и рисуем рамку\n",
    "                self._draw(frame, boxes, probs, landmarks)\n",
    "                \n",
    "                # Если в кадре есть лицо, то считываем жест:\n",
    "                if boxes is not None:\n",
    "                     \n",
    "                    # детектируем расположение рук на кадре\n",
    "                    width, height, inference_time, results = self.hand_detector.inference(frame)\n",
    "\n",
    "                    boxes_hand = []\n",
    "                    for detection in results:\n",
    "                        id, name, confidence, x, y, w, h = detection\n",
    "                        boxes_hand = [[x, y, x + w, y + h]]\n",
    "\n",
    "\n",
    "                    # Вырезаем руку из кадра с некоторым захватом области exp=30\n",
    "                    hand = self.crop_hand(frame, boxes_hand, exp=30)[0]\n",
    "                    # Меняем размер изображения руки для входа в нейронную сеть\n",
    "                    hand = cv2.resize(hand,(100,100))\n",
    "                    # Превращаем в 1-канальное серое изображение\n",
    "                    hand = cv2.cvtColor(hand, cv2.COLOR_BGR2GRAY)\n",
    "                    # Превращаем numpy-картинку вырезанной руки в pytorch-тензор\n",
    "                    torch_hand = torch.from_numpy(hand).unsqueeze(0).to(self.device).float()\n",
    "\n",
    "                    # Загужаем наш тензор руки в нейронную сеть и получаем предсказание\n",
    "                    gesture = self.emodel(torch_hand[None, ...])               \n",
    "                    # Заполняем таблици жестов\n",
    "                    guess_list[int(gesture.argmax())] += 1\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "                #print('Something wrong im main cycle!')\n",
    "            \n",
    "            # Получаем жест с наибольшим значением \n",
    "            if max(guess_list) >= 4:\n",
    "                gesture_idx = guess_list.argmax()\n",
    "                guess_list = np.array([0] * 10)\n",
    "            \n",
    "            # Реакция на наш жест (вывод названия жеста на экран и изменение параметров выводимого кадра)\n",
    "            gesture_to_show, changed_frame = self.gesture_action(gesture_idx, frame)\n",
    "            cv2.putText(changed_frame, gesture_to_show, (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (188, 198, 48), 2, cv2.LINE_AA)\n",
    "            # Показываем кадр в окне, и назвываем его(окно) - 'Gesture Recognition'\n",
    "            cv2.imshow('Gesture Recognition', changed_frame)\n",
    "            \n",
    "            # Функция, которая проверяет нажатие на клавишу 'q'\n",
    "            # Если нажатие произошло - выход из цикла. Конец работы приложения\n",
    "            if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "                break\n",
    "\n",
    "        # Очищаем все объекты opencv, что мы создали\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "# Загружаем мтцнн\n",
    "mtcnn = MTCNN()\n",
    "# Создаем объект нашего класса приложения\n",
    "fcd = FaceDetector(mtcnn)\n",
    "# Запускаем\n",
    "fcd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
