{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 2. CNN and LSTM for human action recognition#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -- Автор: Шенк Евгений Станиславович"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "Необходимо нормализовать данные в датасете: написать функцию нормализации и использовать ее внутри класса датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выполнение:  \n",
    "    - Созданы 2 класса:\n",
    "            нормализация mean_std: (x - x.mean()) / x.std()\n",
    "            нормализация min-max: (x - x.min()) / (x.max() - x.min()\n",
    "    - На датасете из урока получилось, что нормализация mean_std улучшает результат, а min-max ухудшает, а в Задании 2 нормализация особо на результат не влияла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeletons = pd.read_csv(\"skels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {0: \"cheer up\", 1: \"jump up\", 2:  \"hand waving\", 3: \"sitting down\", 4: \"clapping\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(636, 3376)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skeletons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>...</th>\n",
       "      <th>3126</th>\n",
       "      <th>3127</th>\n",
       "      <th>3128</th>\n",
       "      <th>3129</th>\n",
       "      <th>3130</th>\n",
       "      <th>3131</th>\n",
       "      <th>3132</th>\n",
       "      <th>3133</th>\n",
       "      <th>3134</th>\n",
       "      <th>3135</th>\n",
       "      <th>3136</th>\n",
       "      <th>3137</th>\n",
       "      <th>3138</th>\n",
       "      <th>3139</th>\n",
       "      <th>3140</th>\n",
       "      <th>3141</th>\n",
       "      <th>3142</th>\n",
       "      <th>3143</th>\n",
       "      <th>3144</th>\n",
       "      <th>3145</th>\n",
       "      <th>3146</th>\n",
       "      <th>3147</th>\n",
       "      <th>3148</th>\n",
       "      <th>3149</th>\n",
       "      <th>3150</th>\n",
       "      <th>3151</th>\n",
       "      <th>3152</th>\n",
       "      <th>3153</th>\n",
       "      <th>3154</th>\n",
       "      <th>3155</th>\n",
       "      <th>3156</th>\n",
       "      <th>3157</th>\n",
       "      <th>3158</th>\n",
       "      <th>3159</th>\n",
       "      <th>3160</th>\n",
       "      <th>3161</th>\n",
       "      <th>3162</th>\n",
       "      <th>3163</th>\n",
       "      <th>3164</th>\n",
       "      <th>3165</th>\n",
       "      <th>3166</th>\n",
       "      <th>3167</th>\n",
       "      <th>3168</th>\n",
       "      <th>3169</th>\n",
       "      <th>3170</th>\n",
       "      <th>3171</th>\n",
       "      <th>3172</th>\n",
       "      <th>3173</th>\n",
       "      <th>3174</th>\n",
       "      <th>3175</th>\n",
       "      <th>3176</th>\n",
       "      <th>3177</th>\n",
       "      <th>3178</th>\n",
       "      <th>3179</th>\n",
       "      <th>3180</th>\n",
       "      <th>3181</th>\n",
       "      <th>3182</th>\n",
       "      <th>3183</th>\n",
       "      <th>3184</th>\n",
       "      <th>3185</th>\n",
       "      <th>3186</th>\n",
       "      <th>3187</th>\n",
       "      <th>3188</th>\n",
       "      <th>3189</th>\n",
       "      <th>3190</th>\n",
       "      <th>3191</th>\n",
       "      <th>3192</th>\n",
       "      <th>3193</th>\n",
       "      <th>3194</th>\n",
       "      <th>3195</th>\n",
       "      <th>3196</th>\n",
       "      <th>3197</th>\n",
       "      <th>3198</th>\n",
       "      <th>3199</th>\n",
       "      <th>3200</th>\n",
       "      <th>3201</th>\n",
       "      <th>3202</th>\n",
       "      <th>3203</th>\n",
       "      <th>3204</th>\n",
       "      <th>3205</th>\n",
       "      <th>3206</th>\n",
       "      <th>3207</th>\n",
       "      <th>3208</th>\n",
       "      <th>3209</th>\n",
       "      <th>3210</th>\n",
       "      <th>3211</th>\n",
       "      <th>3212</th>\n",
       "      <th>3213</th>\n",
       "      <th>3214</th>\n",
       "      <th>3215</th>\n",
       "      <th>3216</th>\n",
       "      <th>3217</th>\n",
       "      <th>3218</th>\n",
       "      <th>3219</th>\n",
       "      <th>3220</th>\n",
       "      <th>3221</th>\n",
       "      <th>3222</th>\n",
       "      <th>3223</th>\n",
       "      <th>3224</th>\n",
       "      <th>3225</th>\n",
       "      <th>3226</th>\n",
       "      <th>3227</th>\n",
       "      <th>3228</th>\n",
       "      <th>3229</th>\n",
       "      <th>3230</th>\n",
       "      <th>3231</th>\n",
       "      <th>3232</th>\n",
       "      <th>3233</th>\n",
       "      <th>3234</th>\n",
       "      <th>3235</th>\n",
       "      <th>3236</th>\n",
       "      <th>3237</th>\n",
       "      <th>3238</th>\n",
       "      <th>3239</th>\n",
       "      <th>3240</th>\n",
       "      <th>3241</th>\n",
       "      <th>3242</th>\n",
       "      <th>3243</th>\n",
       "      <th>3244</th>\n",
       "      <th>3245</th>\n",
       "      <th>3246</th>\n",
       "      <th>3247</th>\n",
       "      <th>3248</th>\n",
       "      <th>3249</th>\n",
       "      <th>3250</th>\n",
       "      <th>3251</th>\n",
       "      <th>3252</th>\n",
       "      <th>3253</th>\n",
       "      <th>3254</th>\n",
       "      <th>3255</th>\n",
       "      <th>3256</th>\n",
       "      <th>3257</th>\n",
       "      <th>3258</th>\n",
       "      <th>3259</th>\n",
       "      <th>3260</th>\n",
       "      <th>3261</th>\n",
       "      <th>3262</th>\n",
       "      <th>3263</th>\n",
       "      <th>3264</th>\n",
       "      <th>3265</th>\n",
       "      <th>3266</th>\n",
       "      <th>3267</th>\n",
       "      <th>3268</th>\n",
       "      <th>3269</th>\n",
       "      <th>3270</th>\n",
       "      <th>3271</th>\n",
       "      <th>3272</th>\n",
       "      <th>3273</th>\n",
       "      <th>3274</th>\n",
       "      <th>3275</th>\n",
       "      <th>3276</th>\n",
       "      <th>3277</th>\n",
       "      <th>3278</th>\n",
       "      <th>3279</th>\n",
       "      <th>3280</th>\n",
       "      <th>3281</th>\n",
       "      <th>3282</th>\n",
       "      <th>3283</th>\n",
       "      <th>3284</th>\n",
       "      <th>3285</th>\n",
       "      <th>3286</th>\n",
       "      <th>3287</th>\n",
       "      <th>3288</th>\n",
       "      <th>3289</th>\n",
       "      <th>3290</th>\n",
       "      <th>3291</th>\n",
       "      <th>3292</th>\n",
       "      <th>3293</th>\n",
       "      <th>3294</th>\n",
       "      <th>3295</th>\n",
       "      <th>3296</th>\n",
       "      <th>3297</th>\n",
       "      <th>3298</th>\n",
       "      <th>3299</th>\n",
       "      <th>3300</th>\n",
       "      <th>3301</th>\n",
       "      <th>3302</th>\n",
       "      <th>3303</th>\n",
       "      <th>3304</th>\n",
       "      <th>3305</th>\n",
       "      <th>3306</th>\n",
       "      <th>3307</th>\n",
       "      <th>3308</th>\n",
       "      <th>3309</th>\n",
       "      <th>3310</th>\n",
       "      <th>3311</th>\n",
       "      <th>3312</th>\n",
       "      <th>3313</th>\n",
       "      <th>3314</th>\n",
       "      <th>3315</th>\n",
       "      <th>3316</th>\n",
       "      <th>3317</th>\n",
       "      <th>3318</th>\n",
       "      <th>3319</th>\n",
       "      <th>3320</th>\n",
       "      <th>3321</th>\n",
       "      <th>3322</th>\n",
       "      <th>3323</th>\n",
       "      <th>3324</th>\n",
       "      <th>3325</th>\n",
       "      <th>3326</th>\n",
       "      <th>3327</th>\n",
       "      <th>3328</th>\n",
       "      <th>3329</th>\n",
       "      <th>3330</th>\n",
       "      <th>3331</th>\n",
       "      <th>3332</th>\n",
       "      <th>3333</th>\n",
       "      <th>3334</th>\n",
       "      <th>3335</th>\n",
       "      <th>3336</th>\n",
       "      <th>3337</th>\n",
       "      <th>3338</th>\n",
       "      <th>3339</th>\n",
       "      <th>3340</th>\n",
       "      <th>3341</th>\n",
       "      <th>3342</th>\n",
       "      <th>3343</th>\n",
       "      <th>3344</th>\n",
       "      <th>3345</th>\n",
       "      <th>3346</th>\n",
       "      <th>3347</th>\n",
       "      <th>3348</th>\n",
       "      <th>3349</th>\n",
       "      <th>3350</th>\n",
       "      <th>3351</th>\n",
       "      <th>3352</th>\n",
       "      <th>3353</th>\n",
       "      <th>3354</th>\n",
       "      <th>3355</th>\n",
       "      <th>3356</th>\n",
       "      <th>3357</th>\n",
       "      <th>3358</th>\n",
       "      <th>3359</th>\n",
       "      <th>3360</th>\n",
       "      <th>3361</th>\n",
       "      <th>3362</th>\n",
       "      <th>3363</th>\n",
       "      <th>3364</th>\n",
       "      <th>3365</th>\n",
       "      <th>3366</th>\n",
       "      <th>3367</th>\n",
       "      <th>3368</th>\n",
       "      <th>3369</th>\n",
       "      <th>3370</th>\n",
       "      <th>3371</th>\n",
       "      <th>3372</th>\n",
       "      <th>3373</th>\n",
       "      <th>3374</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.526048</td>\n",
       "      <td>-0.277147</td>\n",
       "      <td>2.987706</td>\n",
       "      <td>-0.606184</td>\n",
       "      <td>-0.010056</td>\n",
       "      <td>3.010000</td>\n",
       "      <td>-0.681454</td>\n",
       "      <td>0.251614</td>\n",
       "      <td>3.020046</td>\n",
       "      <td>-0.708330</td>\n",
       "      <td>0.386629</td>\n",
       "      <td>3.087164</td>\n",
       "      <td>-0.704835</td>\n",
       "      <td>0.111742</td>\n",
       "      <td>2.856311</td>\n",
       "      <td>-0.515683</td>\n",
       "      <td>-0.024701</td>\n",
       "      <td>2.727949</td>\n",
       "      <td>-0.353615</td>\n",
       "      <td>-0.123308</td>\n",
       "      <td>2.706112</td>\n",
       "      <td>-0.327895</td>\n",
       "      <td>-0.141611</td>\n",
       "      <td>2.749221</td>\n",
       "      <td>-0.511823</td>\n",
       "      <td>0.15791</td>\n",
       "      <td>3.311851</td>\n",
       "      <td>-0.543163</td>\n",
       "      <td>-0.013495</td>\n",
       "      <td>3.108863</td>\n",
       "      <td>-0.496480</td>\n",
       "      <td>-0.181723</td>\n",
       "      <td>2.912678</td>\n",
       "      <td>-0.424105</td>\n",
       "      <td>-0.341968</td>\n",
       "      <td>2.685757</td>\n",
       "      <td>-0.559787</td>\n",
       "      <td>-0.277896</td>\n",
       "      <td>2.867135</td>\n",
       "      <td>-0.187794</td>\n",
       "      <td>-0.440893</td>\n",
       "      <td>2.817494</td>\n",
       "      <td>-0.051320</td>\n",
       "      <td>-0.741197</td>\n",
       "      <td>2.696164</td>\n",
       "      <td>0.035544</td>\n",
       "      <td>-0.772924</td>\n",
       "      <td>2.653029</td>\n",
       "      <td>-0.479626</td>\n",
       "      <td>-0.266888</td>\n",
       "      <td>3.033314</td>\n",
       "      <td>-0.132970</td>\n",
       "      <td>-0.302009</td>\n",
       "      <td>2.766374</td>\n",
       "      <td>-0.026497</td>\n",
       "      <td>-0.672848</td>\n",
       "      <td>3.018918</td>\n",
       "      <td>0.066964</td>\n",
       "      <td>-0.706324</td>\n",
       "      <td>2.934041</td>\n",
       "      <td>-0.663448</td>\n",
       "      <td>0.187231</td>\n",
       "      <td>3.019821</td>\n",
       "      <td>-0.287726</td>\n",
       "      <td>-0.173169</td>\n",
       "      <td>2.772417</td>\n",
       "      <td>-0.374481</td>\n",
       "      <td>-0.201329</td>\n",
       "      <td>2.647500</td>\n",
       "      <td>-0.435660</td>\n",
       "      <td>-0.399829</td>\n",
       "      <td>2.653183</td>\n",
       "      <td>-0.382046</td>\n",
       "      <td>-0.341303</td>\n",
       "      <td>2.709815</td>\n",
       "      <td>-0.522987</td>\n",
       "      <td>-0.302266</td>\n",
       "      <td>2.985698</td>\n",
       "      <td>-0.606354</td>\n",
       "      <td>-0.018129</td>\n",
       "      <td>3.009436</td>\n",
       "      <td>-0.682353</td>\n",
       "      <td>0.250679</td>\n",
       "      <td>3.019942</td>\n",
       "      <td>-0.708845</td>\n",
       "      <td>0.385737</td>\n",
       "      <td>3.086951</td>\n",
       "      <td>-0.703320</td>\n",
       "      <td>0.110657</td>\n",
       "      <td>2.854790</td>\n",
       "      <td>-0.514419</td>\n",
       "      <td>-0.026362</td>\n",
       "      <td>2.726717</td>\n",
       "      <td>-0.351712</td>\n",
       "      <td>-0.124065</td>\n",
       "      <td>2.70648</td>\n",
       "      <td>-0.325711</td>\n",
       "      <td>-0.143484</td>\n",
       "      <td>2.749665</td>\n",
       "      <td>-0.512720</td>\n",
       "      <td>0.156979</td>\n",
       "      <td>3.311748</td>\n",
       "      <td>-0.544056</td>\n",
       "      <td>-0.014422</td>\n",
       "      <td>3.108759</td>\n",
       "      <td>-0.496896</td>\n",
       "      <td>-0.182398</td>\n",
       "      <td>2.912465</td>\n",
       "      <td>-0.422955</td>\n",
       "      <td>-0.339052</td>\n",
       "      <td>2.685333</td>\n",
       "      <td>-0.555938</td>\n",
       "      <td>-0.311383</td>\n",
       "      <td>2.888311</td>\n",
       "      <td>-0.187798</td>\n",
       "      <td>-0.440844</td>\n",
       "      <td>2.817507</td>\n",
       "      <td>-0.051158</td>\n",
       "      <td>-0.741131</td>\n",
       "      <td>2.696049</td>\n",
       "      <td>0.036761</td>\n",
       "      <td>-0.770821</td>\n",
       "      <td>2.654296</td>\n",
       "      <td>-0.475196</td>\n",
       "      <td>-0.299777</td>\n",
       "      <td>3.005829</td>\n",
       "      <td>-0.132759</td>\n",
       "      <td>-0.301510</td>\n",
       "      <td>2.766153</td>\n",
       "      <td>-0.026572</td>\n",
       "      <td>-0.672512</td>\n",
       "      <td>3.018797</td>\n",
       "      <td>0.067243</td>\n",
       "      <td>-0.705909</td>\n",
       "      <td>2.934533</td>\n",
       "      <td>-0.664326</td>\n",
       "      <td>0.185232</td>\n",
       "      <td>3.019666</td>\n",
       "      <td>-0.286222</td>\n",
       "      <td>-0.174110</td>\n",
       "      <td>2.770132</td>\n",
       "      <td>-0.344511</td>\n",
       "      <td>-0.206511</td>\n",
       "      <td>2.714750</td>\n",
       "      <td>-0.432879</td>\n",
       "      <td>-0.389477</td>\n",
       "      <td>2.647004</td>\n",
       "      <td>-0.383055</td>\n",
       "      <td>-0.342364</td>\n",
       "      <td>2.709956</td>\n",
       "      <td>-0.522926</td>\n",
       "      <td>-0.291120</td>\n",
       "      <td>2.983348</td>\n",
       "      <td>-0.606446</td>\n",
       "      <td>-0.017055</td>\n",
       "      <td>3.008816</td>\n",
       "      <td>-0.682771</td>\n",
       "      <td>0.250588</td>\n",
       "      <td>3.019696</td>\n",
       "      <td>-0.709493</td>\n",
       "      <td>0.385637</td>\n",
       "      <td>3.086540</td>\n",
       "      <td>-0.703649</td>\n",
       "      <td>0.110274</td>\n",
       "      <td>2.853598</td>\n",
       "      <td>-0.515603</td>\n",
       "      <td>-0.027185</td>\n",
       "      <td>2.724664</td>\n",
       "      <td>-0.352023</td>\n",
       "      <td>-0.123337</td>\n",
       "      <td>2.722028</td>\n",
       "      <td>-0.318310</td>\n",
       "      <td>-0.146859</td>\n",
       "      <td>2.754510</td>\n",
       "      <td>-0.513136</td>\n",
       "      <td>0.156888</td>\n",
       "      <td>3.311502</td>\n",
       "      <td>-0.544470</td>\n",
       "      <td>-0.014513</td>\n",
       "      <td>3.108513</td>\n",
       "      <td>-0.497013</td>\n",
       "      <td>-0.182307</td>\n",
       "      <td>2.912128</td>\n",
       "      <td>-0.423729</td>\n",
       "      <td>-0.339591</td>\n",
       "      <td>2.684088</td>\n",
       "      <td>-0.557001</td>\n",
       "      <td>-0.279047</td>\n",
       "      <td>2.854529</td>\n",
       "      <td>-0.187803</td>\n",
       "      <td>-0.440799</td>\n",
       "      <td>2.817520</td>\n",
       "      <td>-0.050492</td>\n",
       "      <td>-0.740845</td>\n",
       "      <td>2.695943</td>\n",
       "      <td>0.036925</td>\n",
       "      <td>-0.771311</td>\n",
       "      <td>2.652386</td>\n",
       "      <td>-0.47548</td>\n",
       "      <td>-0.278758</td>\n",
       "      <td>3.026471</td>\n",
       "      <td>-0.131031</td>\n",
       "      <td>-0.301038</td>\n",
       "      <td>2.763053</td>\n",
       "      <td>-0.026342</td>\n",
       "      <td>-0.671314</td>\n",
       "      <td>3.017473</td>\n",
       "      <td>0.067367</td>\n",
       "      <td>-0.704880</td>\n",
       "      <td>2.933144</td>\n",
       "      <td>-0.664581</td>\n",
       "      <td>0.185240</td>\n",
       "      <td>3.019431</td>\n",
       "      <td>-0.238680</td>\n",
       "      <td>-0.199818</td>\n",
       "      <td>2.774265</td>\n",
       "      <td>-0.309908</td>\n",
       "      <td>-0.195522</td>\n",
       "      <td>2.768333</td>\n",
       "      <td>-0.438753</td>\n",
       "      <td>-0.393265</td>\n",
       "      <td>2.647284</td>\n",
       "      <td>-0.381671</td>\n",
       "      <td>-0.341660</td>\n",
       "      <td>2.709762</td>\n",
       "      <td>-0.521120</td>\n",
       "      <td>-0.298138</td>\n",
       "      <td>2.983961</td>\n",
       "      <td>-0.606013</td>\n",
       "      <td>-0.017783</td>\n",
       "      <td>3.008922</td>\n",
       "      <td>-0.682892</td>\n",
       "      <td>0.251654</td>\n",
       "      <td>3.019938</td>\n",
       "      <td>-0.711226</td>\n",
       "      <td>0.386574</td>\n",
       "      <td>3.086356</td>\n",
       "      <td>-0.704142</td>\n",
       "      <td>0.110154</td>\n",
       "      <td>2.853352</td>\n",
       "      <td>-0.524689</td>\n",
       "      <td>-0.034344</td>\n",
       "      <td>2.719633</td>\n",
       "      <td>-0.305559</td>\n",
       "      <td>-0.157328</td>\n",
       "      <td>2.758009</td>\n",
       "      <td>-0.257899</td>\n",
       "      <td>-0.223257</td>\n",
       "      <td>2.782157</td>\n",
       "      <td>-0.513257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117457</td>\n",
       "      <td>-0.297415</td>\n",
       "      <td>2.760189</td>\n",
       "      <td>-0.025414</td>\n",
       "      <td>-0.669564</td>\n",
       "      <td>3.016903</td>\n",
       "      <td>0.064981</td>\n",
       "      <td>-0.702720</td>\n",
       "      <td>2.926161</td>\n",
       "      <td>-0.737668</td>\n",
       "      <td>0.232860</td>\n",
       "      <td>2.969651</td>\n",
       "      <td>-0.700851</td>\n",
       "      <td>0.599396</td>\n",
       "      <td>2.785406</td>\n",
       "      <td>-0.697228</td>\n",
       "      <td>0.531868</td>\n",
       "      <td>2.74700</td>\n",
       "      <td>-0.642330</td>\n",
       "      <td>0.376903</td>\n",
       "      <td>3.053223</td>\n",
       "      <td>-0.643093</td>\n",
       "      <td>0.381854</td>\n",
       "      <td>3.045156</td>\n",
       "      <td>-0.536747</td>\n",
       "      <td>-0.268661</td>\n",
       "      <td>3.019472</td>\n",
       "      <td>-0.651120</td>\n",
       "      <td>0.016701</td>\n",
       "      <td>2.996562</td>\n",
       "      <td>-0.765691</td>\n",
       "      <td>0.305620</td>\n",
       "      <td>2.962245</td>\n",
       "      <td>-0.793885</td>\n",
       "      <td>0.438116</td>\n",
       "      <td>3.046815</td>\n",
       "      <td>-0.760563</td>\n",
       "      <td>0.104213</td>\n",
       "      <td>2.855314</td>\n",
       "      <td>-0.655104</td>\n",
       "      <td>0.219293</td>\n",
       "      <td>2.729084</td>\n",
       "      <td>-0.641481</td>\n",
       "      <td>0.472357</td>\n",
       "      <td>2.719059</td>\n",
       "      <td>-0.662671</td>\n",
       "      <td>0.529586</td>\n",
       "      <td>2.742141</td>\n",
       "      <td>-0.595748</td>\n",
       "      <td>0.211718</td>\n",
       "      <td>3.254298</td>\n",
       "      <td>-0.651054</td>\n",
       "      <td>0.271530</td>\n",
       "      <td>3.402166</td>\n",
       "      <td>-0.646946</td>\n",
       "      <td>0.347994</td>\n",
       "      <td>3.149582</td>\n",
       "      <td>-0.646372</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>3.069490</td>\n",
       "      <td>-0.575384</td>\n",
       "      <td>-0.279854</td>\n",
       "      <td>2.961231</td>\n",
       "      <td>-0.254864</td>\n",
       "      <td>-0.200216</td>\n",
       "      <td>2.798510</td>\n",
       "      <td>-0.091002</td>\n",
       "      <td>-0.637781</td>\n",
       "      <td>2.719589</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>-0.670706</td>\n",
       "      <td>2.630121</td>\n",
       "      <td>-0.484576</td>\n",
       "      <td>-0.250839</td>\n",
       "      <td>3.007088</td>\n",
       "      <td>-0.141068</td>\n",
       "      <td>-0.346020</td>\n",
       "      <td>2.762192</td>\n",
       "      <td>-0.024301</td>\n",
       "      <td>-0.699762</td>\n",
       "      <td>3.033093</td>\n",
       "      <td>0.066705</td>\n",
       "      <td>-0.732814</td>\n",
       "      <td>2.943577</td>\n",
       "      <td>-0.737606</td>\n",
       "      <td>0.233676</td>\n",
       "      <td>2.972858</td>\n",
       "      <td>-0.707763</td>\n",
       "      <td>0.589335</td>\n",
       "      <td>2.780041</td>\n",
       "      <td>-0.678035</td>\n",
       "      <td>0.497967</td>\n",
       "      <td>2.730250</td>\n",
       "      <td>-0.642994</td>\n",
       "      <td>0.380211</td>\n",
       "      <td>3.053722</td>\n",
       "      <td>-0.643756</td>\n",
       "      <td>0.385148</td>\n",
       "      <td>3.045661</td>\n",
       "      <td>-0.536393</td>\n",
       "      <td>-0.269593</td>\n",
       "      <td>3.019098</td>\n",
       "      <td>-0.650550</td>\n",
       "      <td>0.015861</td>\n",
       "      <td>2.994569</td>\n",
       "      <td>-0.761782</td>\n",
       "      <td>0.300374</td>\n",
       "      <td>2.954222</td>\n",
       "      <td>-0.794149</td>\n",
       "      <td>0.430042</td>\n",
       "      <td>3.045948</td>\n",
       "      <td>-0.760232</td>\n",
       "      <td>0.105684</td>\n",
       "      <td>2.853519</td>\n",
       "      <td>-0.640828</td>\n",
       "      <td>0.247795</td>\n",
       "      <td>2.727601</td>\n",
       "      <td>-0.636687</td>\n",
       "      <td>0.500929</td>\n",
       "      <td>2.718496</td>\n",
       "      <td>-0.640456</td>\n",
       "      <td>0.596024</td>\n",
       "      <td>2.740083</td>\n",
       "      <td>-0.591833</td>\n",
       "      <td>0.206482</td>\n",
       "      <td>3.246271</td>\n",
       "      <td>-0.682753</td>\n",
       "      <td>0.257004</td>\n",
       "      <td>3.380537</td>\n",
       "      <td>-0.662777</td>\n",
       "      <td>0.361562</td>\n",
       "      <td>3.139406</td>\n",
       "      <td>-0.647476</td>\n",
       "      <td>0.609547</td>\n",
       "      <td>2.747746</td>\n",
       "      <td>-0.575439</td>\n",
       "      <td>-0.280800</td>\n",
       "      <td>2.960728</td>\n",
       "      <td>-0.254915</td>\n",
       "      <td>-0.199929</td>\n",
       "      <td>2.798599</td>\n",
       "      <td>-0.091137</td>\n",
       "      <td>-0.637441</td>\n",
       "      <td>2.719661</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>-0.670160</td>\n",
       "      <td>2.629707</td>\n",
       "      <td>-0.483578</td>\n",
       "      <td>-0.252083</td>\n",
       "      <td>3.005907</td>\n",
       "      <td>-0.142665</td>\n",
       "      <td>-0.296575</td>\n",
       "      <td>2.762277</td>\n",
       "      <td>-0.031544</td>\n",
       "      <td>-0.666042</td>\n",
       "      <td>3.015459</td>\n",
       "      <td>0.059394</td>\n",
       "      <td>-0.699224</td>\n",
       "      <td>2.925381</td>\n",
       "      <td>-0.735025</td>\n",
       "      <td>0.230247</td>\n",
       "      <td>2.967056</td>\n",
       "      <td>-0.647058</td>\n",
       "      <td>0.645758</td>\n",
       "      <td>2.747567</td>\n",
       "      <td>-0.669871</td>\n",
       "      <td>0.607714</td>\n",
       "      <td>2.749000</td>\n",
       "      <td>-0.631756</td>\n",
       "      <td>0.626769</td>\n",
       "      <td>2.731108</td>\n",
       "      <td>-0.653891</td>\n",
       "      <td>0.64260</td>\n",
       "      <td>2.740333</td>\n",
       "      <td>-0.535844</td>\n",
       "      <td>-0.287107</td>\n",
       "      <td>3.009541</td>\n",
       "      <td>-0.641248</td>\n",
       "      <td>-0.016169</td>\n",
       "      <td>2.989267</td>\n",
       "      <td>-0.737427</td>\n",
       "      <td>0.244177</td>\n",
       "      <td>2.959609</td>\n",
       "      <td>-0.789884</td>\n",
       "      <td>0.407848</td>\n",
       "      <td>3.048982</td>\n",
       "      <td>-0.729282</td>\n",
       "      <td>0.171244</td>\n",
       "      <td>2.829513</td>\n",
       "      <td>-0.625358</td>\n",
       "      <td>0.308465</td>\n",
       "      <td>2.731224</td>\n",
       "      <td>-0.586710</td>\n",
       "      <td>0.542867</td>\n",
       "      <td>2.691673</td>\n",
       "      <td>-0.596145</td>\n",
       "      <td>0.636903</td>\n",
       "      <td>2.709147</td>\n",
       "      <td>-0.567538</td>\n",
       "      <td>0.150466</td>\n",
       "      <td>3.251560</td>\n",
       "      <td>-0.574421</td>\n",
       "      <td>0.303848</td>\n",
       "      <td>3.075653</td>\n",
       "      <td>-0.596376</td>\n",
       "      <td>0.471110</td>\n",
       "      <td>2.875680</td>\n",
       "      <td>-0.589655</td>\n",
       "      <td>0.586514</td>\n",
       "      <td>2.717145</td>\n",
       "      <td>-0.570154</td>\n",
       "      <td>-0.290760</td>\n",
       "      <td>2.955790</td>\n",
       "      <td>-0.255583</td>\n",
       "      <td>-0.187767</td>\n",
       "      <td>2.804032</td>\n",
       "      <td>-0.094777</td>\n",
       "      <td>-0.625936</td>\n",
       "      <td>2.721997</td>\n",
       "      <td>0.002962</td>\n",
       "      <td>-0.660430</td>\n",
       "      <td>2.645224</td>\n",
       "      <td>-0.491591</td>\n",
       "      <td>-0.281821</td>\n",
       "      <td>2.985380</td>\n",
       "      <td>-0.109749</td>\n",
       "      <td>-0.353734</td>\n",
       "      <td>2.758277</td>\n",
       "      <td>-0.016498</td>\n",
       "      <td>-0.707223</td>\n",
       "      <td>3.037502</td>\n",
       "      <td>0.080172</td>\n",
       "      <td>-0.741614</td>\n",
       "      <td>2.960732</td>\n",
       "      <td>-0.713666</td>\n",
       "      <td>0.178025</td>\n",
       "      <td>2.969061</td>\n",
       "      <td>-0.599133</td>\n",
       "      <td>0.702776</td>\n",
       "      <td>2.722800</td>\n",
       "      <td>-0.619659</td>\n",
       "      <td>0.647852</td>\n",
       "      <td>2.708625</td>\n",
       "      <td>-0.598600</td>\n",
       "      <td>0.593635</td>\n",
       "      <td>2.707864</td>\n",
       "      <td>-0.601837</td>\n",
       "      <td>0.592191</td>\n",
       "      <td>2.697301</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.325320</td>\n",
       "      <td>-0.560200</td>\n",
       "      <td>3.244968</td>\n",
       "      <td>-0.296753</td>\n",
       "      <td>-0.312155</td>\n",
       "      <td>3.234485</td>\n",
       "      <td>-0.267607</td>\n",
       "      <td>-0.066345</td>\n",
       "      <td>3.212031</td>\n",
       "      <td>-0.286594</td>\n",
       "      <td>0.049026</td>\n",
       "      <td>3.222926</td>\n",
       "      <td>-0.412259</td>\n",
       "      <td>-0.156479</td>\n",
       "      <td>3.294798</td>\n",
       "      <td>-0.467307</td>\n",
       "      <td>-0.373163</td>\n",
       "      <td>3.353605</td>\n",
       "      <td>-0.452381</td>\n",
       "      <td>-0.607216</td>\n",
       "      <td>3.276438</td>\n",
       "      <td>-0.459716</td>\n",
       "      <td>-0.668793</td>\n",
       "      <td>3.282482</td>\n",
       "      <td>-0.160399</td>\n",
       "      <td>-0.17987</td>\n",
       "      <td>3.150078</td>\n",
       "      <td>-0.136725</td>\n",
       "      <td>-0.403121</td>\n",
       "      <td>3.128864</td>\n",
       "      <td>-0.160573</td>\n",
       "      <td>-0.601392</td>\n",
       "      <td>3.074458</td>\n",
       "      <td>-0.177965</td>\n",
       "      <td>-0.641335</td>\n",
       "      <td>3.074898</td>\n",
       "      <td>-0.368195</td>\n",
       "      <td>-0.549674</td>\n",
       "      <td>3.236222</td>\n",
       "      <td>-0.341016</td>\n",
       "      <td>-0.866046</td>\n",
       "      <td>3.282849</td>\n",
       "      <td>-0.313741</td>\n",
       "      <td>-1.186290</td>\n",
       "      <td>3.351244</td>\n",
       "      <td>-0.348155</td>\n",
       "      <td>-1.212369</td>\n",
       "      <td>3.259632</td>\n",
       "      <td>-0.276780</td>\n",
       "      <td>-0.560417</td>\n",
       "      <td>3.195294</td>\n",
       "      <td>-0.206566</td>\n",
       "      <td>-0.869908</td>\n",
       "      <td>3.187913</td>\n",
       "      <td>-0.176944</td>\n",
       "      <td>-1.191492</td>\n",
       "      <td>3.236771</td>\n",
       "      <td>-0.216591</td>\n",
       "      <td>-1.252620</td>\n",
       "      <td>3.239565</td>\n",
       "      <td>-0.275193</td>\n",
       "      <td>-0.127509</td>\n",
       "      <td>3.219691</td>\n",
       "      <td>-0.432343</td>\n",
       "      <td>-0.703650</td>\n",
       "      <td>3.251001</td>\n",
       "      <td>-0.433775</td>\n",
       "      <td>-0.662048</td>\n",
       "      <td>3.229998</td>\n",
       "      <td>-0.182828</td>\n",
       "      <td>-0.695215</td>\n",
       "      <td>3.070847</td>\n",
       "      <td>-0.201468</td>\n",
       "      <td>-0.627444</td>\n",
       "      <td>3.066871</td>\n",
       "      <td>-0.325523</td>\n",
       "      <td>-0.560813</td>\n",
       "      <td>3.245201</td>\n",
       "      <td>-0.297902</td>\n",
       "      <td>-0.313838</td>\n",
       "      <td>3.234329</td>\n",
       "      <td>-0.270614</td>\n",
       "      <td>-0.070106</td>\n",
       "      <td>3.211387</td>\n",
       "      <td>-0.287911</td>\n",
       "      <td>0.048311</td>\n",
       "      <td>3.222532</td>\n",
       "      <td>-0.412941</td>\n",
       "      <td>-0.157385</td>\n",
       "      <td>3.293802</td>\n",
       "      <td>-0.467792</td>\n",
       "      <td>-0.373858</td>\n",
       "      <td>3.352457</td>\n",
       "      <td>-0.452864</td>\n",
       "      <td>-0.607909</td>\n",
       "      <td>3.27529</td>\n",
       "      <td>-0.463337</td>\n",
       "      <td>-0.670728</td>\n",
       "      <td>3.287665</td>\n",
       "      <td>-0.160529</td>\n",
       "      <td>-0.180116</td>\n",
       "      <td>3.150119</td>\n",
       "      <td>-0.136941</td>\n",
       "      <td>-0.403283</td>\n",
       "      <td>3.128651</td>\n",
       "      <td>-0.160401</td>\n",
       "      <td>-0.601312</td>\n",
       "      <td>3.074679</td>\n",
       "      <td>-0.175291</td>\n",
       "      <td>-0.641988</td>\n",
       "      <td>3.075238</td>\n",
       "      <td>-0.368410</td>\n",
       "      <td>-0.550553</td>\n",
       "      <td>3.236241</td>\n",
       "      <td>-0.341223</td>\n",
       "      <td>-0.866905</td>\n",
       "      <td>3.282910</td>\n",
       "      <td>-0.313706</td>\n",
       "      <td>-1.186323</td>\n",
       "      <td>3.351247</td>\n",
       "      <td>-0.347867</td>\n",
       "      <td>-1.212483</td>\n",
       "      <td>3.259542</td>\n",
       "      <td>-0.276990</td>\n",
       "      <td>-0.560928</td>\n",
       "      <td>3.195813</td>\n",
       "      <td>-0.207239</td>\n",
       "      <td>-0.869591</td>\n",
       "      <td>3.188367</td>\n",
       "      <td>-0.177693</td>\n",
       "      <td>-1.190907</td>\n",
       "      <td>3.236062</td>\n",
       "      <td>-0.212907</td>\n",
       "      <td>-1.216706</td>\n",
       "      <td>3.144409</td>\n",
       "      <td>-0.277578</td>\n",
       "      <td>-0.130620</td>\n",
       "      <td>3.219173</td>\n",
       "      <td>-0.424656</td>\n",
       "      <td>-0.707095</td>\n",
       "      <td>3.226497</td>\n",
       "      <td>-0.437192</td>\n",
       "      <td>-0.655549</td>\n",
       "      <td>3.231333</td>\n",
       "      <td>-0.179751</td>\n",
       "      <td>-0.693261</td>\n",
       "      <td>3.062350</td>\n",
       "      <td>-0.202178</td>\n",
       "      <td>-0.627065</td>\n",
       "      <td>3.072383</td>\n",
       "      <td>-0.325853</td>\n",
       "      <td>-0.560916</td>\n",
       "      <td>3.244719</td>\n",
       "      <td>-0.298965</td>\n",
       "      <td>-0.314036</td>\n",
       "      <td>3.233568</td>\n",
       "      <td>-0.272403</td>\n",
       "      <td>-0.070329</td>\n",
       "      <td>3.210728</td>\n",
       "      <td>-0.289920</td>\n",
       "      <td>0.048205</td>\n",
       "      <td>3.221743</td>\n",
       "      <td>-0.413437</td>\n",
       "      <td>-0.157693</td>\n",
       "      <td>3.293599</td>\n",
       "      <td>-0.478337</td>\n",
       "      <td>-0.376818</td>\n",
       "      <td>3.359510</td>\n",
       "      <td>-0.463384</td>\n",
       "      <td>-0.610860</td>\n",
       "      <td>3.282351</td>\n",
       "      <td>-0.464546</td>\n",
       "      <td>-0.669435</td>\n",
       "      <td>3.289201</td>\n",
       "      <td>-0.161764</td>\n",
       "      <td>-0.179915</td>\n",
       "      <td>3.149890</td>\n",
       "      <td>-0.136978</td>\n",
       "      <td>-0.403274</td>\n",
       "      <td>3.128533</td>\n",
       "      <td>-0.160413</td>\n",
       "      <td>-0.601456</td>\n",
       "      <td>3.075129</td>\n",
       "      <td>-0.176269</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>3.076296</td>\n",
       "      <td>-0.368790</td>\n",
       "      <td>-0.550665</td>\n",
       "      <td>3.235907</td>\n",
       "      <td>-0.341253</td>\n",
       "      <td>-0.866930</td>\n",
       "      <td>3.282814</td>\n",
       "      <td>-0.313633</td>\n",
       "      <td>-1.185637</td>\n",
       "      <td>3.351261</td>\n",
       "      <td>-0.347889</td>\n",
       "      <td>-1.211834</td>\n",
       "      <td>3.259632</td>\n",
       "      <td>-0.27723</td>\n",
       "      <td>-0.561029</td>\n",
       "      <td>3.195195</td>\n",
       "      <td>-0.207714</td>\n",
       "      <td>-0.869632</td>\n",
       "      <td>3.188687</td>\n",
       "      <td>-0.177563</td>\n",
       "      <td>-1.190137</td>\n",
       "      <td>3.236064</td>\n",
       "      <td>-0.212873</td>\n",
       "      <td>-1.215974</td>\n",
       "      <td>3.144487</td>\n",
       "      <td>-0.279163</td>\n",
       "      <td>-0.130848</td>\n",
       "      <td>3.218463</td>\n",
       "      <td>-0.426399</td>\n",
       "      <td>-0.709989</td>\n",
       "      <td>3.239608</td>\n",
       "      <td>-0.441820</td>\n",
       "      <td>-0.654967</td>\n",
       "      <td>3.240714</td>\n",
       "      <td>-0.180800</td>\n",
       "      <td>-0.694645</td>\n",
       "      <td>3.067773</td>\n",
       "      <td>-0.201585</td>\n",
       "      <td>-0.628618</td>\n",
       "      <td>3.069148</td>\n",
       "      <td>-0.326308</td>\n",
       "      <td>-0.561649</td>\n",
       "      <td>3.245134</td>\n",
       "      <td>-0.300993</td>\n",
       "      <td>-0.314916</td>\n",
       "      <td>3.233880</td>\n",
       "      <td>-0.276975</td>\n",
       "      <td>-0.071172</td>\n",
       "      <td>3.211081</td>\n",
       "      <td>-0.290219</td>\n",
       "      <td>0.048108</td>\n",
       "      <td>3.221657</td>\n",
       "      <td>-0.417483</td>\n",
       "      <td>-0.161675</td>\n",
       "      <td>3.288854</td>\n",
       "      <td>-0.485017</td>\n",
       "      <td>-0.381054</td>\n",
       "      <td>3.357891</td>\n",
       "      <td>-0.470068</td>\n",
       "      <td>-0.615097</td>\n",
       "      <td>3.280774</td>\n",
       "      <td>-0.470077</td>\n",
       "      <td>-0.670854</td>\n",
       "      <td>3.297618</td>\n",
       "      <td>-0.163560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251845</td>\n",
       "      <td>-0.931483</td>\n",
       "      <td>3.169451</td>\n",
       "      <td>-0.141850</td>\n",
       "      <td>-1.223032</td>\n",
       "      <td>3.245492</td>\n",
       "      <td>-0.170539</td>\n",
       "      <td>-1.264311</td>\n",
       "      <td>3.268951</td>\n",
       "      <td>-0.234057</td>\n",
       "      <td>-0.192967</td>\n",
       "      <td>3.174224</td>\n",
       "      <td>-0.066441</td>\n",
       "      <td>-0.590676</td>\n",
       "      <td>3.226854</td>\n",
       "      <td>-0.089689</td>\n",
       "      <td>-0.546069</td>\n",
       "      <td>3.17625</td>\n",
       "      <td>-0.140819</td>\n",
       "      <td>-0.775892</td>\n",
       "      <td>3.070159</td>\n",
       "      <td>-0.130778</td>\n",
       "      <td>-0.709767</td>\n",
       "      <td>3.030000</td>\n",
       "      <td>-0.244266</td>\n",
       "      <td>-0.595101</td>\n",
       "      <td>3.253821</td>\n",
       "      <td>-0.224001</td>\n",
       "      <td>-0.341765</td>\n",
       "      <td>3.220140</td>\n",
       "      <td>-0.203489</td>\n",
       "      <td>-0.093072</td>\n",
       "      <td>3.174690</td>\n",
       "      <td>-0.211584</td>\n",
       "      <td>0.024953</td>\n",
       "      <td>3.165083</td>\n",
       "      <td>-0.350594</td>\n",
       "      <td>-0.174678</td>\n",
       "      <td>3.246146</td>\n",
       "      <td>-0.357803</td>\n",
       "      <td>-0.419481</td>\n",
       "      <td>3.226108</td>\n",
       "      <td>-0.130104</td>\n",
       "      <td>-0.503322</td>\n",
       "      <td>3.254441</td>\n",
       "      <td>-0.076425</td>\n",
       "      <td>-0.515020</td>\n",
       "      <td>3.219594</td>\n",
       "      <td>-0.119309</td>\n",
       "      <td>-0.210774</td>\n",
       "      <td>3.128807</td>\n",
       "      <td>-0.118202</td>\n",
       "      <td>-0.455402</td>\n",
       "      <td>3.102370</td>\n",
       "      <td>-0.164457</td>\n",
       "      <td>-0.648000</td>\n",
       "      <td>3.061667</td>\n",
       "      <td>-0.165752</td>\n",
       "      <td>-0.683368</td>\n",
       "      <td>3.049949</td>\n",
       "      <td>-0.289343</td>\n",
       "      <td>-0.585349</td>\n",
       "      <td>3.245842</td>\n",
       "      <td>-0.296638</td>\n",
       "      <td>-0.865806</td>\n",
       "      <td>3.206981</td>\n",
       "      <td>-0.262789</td>\n",
       "      <td>-1.214496</td>\n",
       "      <td>3.333538</td>\n",
       "      <td>-0.215802</td>\n",
       "      <td>-1.270810</td>\n",
       "      <td>3.309790</td>\n",
       "      <td>-0.194448</td>\n",
       "      <td>-0.593716</td>\n",
       "      <td>3.203144</td>\n",
       "      <td>-0.230097</td>\n",
       "      <td>-0.918363</td>\n",
       "      <td>3.179335</td>\n",
       "      <td>-0.140089</td>\n",
       "      <td>-1.220089</td>\n",
       "      <td>3.243972</td>\n",
       "      <td>-0.167436</td>\n",
       "      <td>-1.262573</td>\n",
       "      <td>3.268584</td>\n",
       "      <td>-0.208444</td>\n",
       "      <td>-0.154559</td>\n",
       "      <td>3.188059</td>\n",
       "      <td>-0.064566</td>\n",
       "      <td>-0.562389</td>\n",
       "      <td>3.220170</td>\n",
       "      <td>-0.076752</td>\n",
       "      <td>-0.477937</td>\n",
       "      <td>3.203286</td>\n",
       "      <td>-0.179791</td>\n",
       "      <td>-0.734700</td>\n",
       "      <td>3.057266</td>\n",
       "      <td>-0.145970</td>\n",
       "      <td>-0.691034</td>\n",
       "      <td>3.035231</td>\n",
       "      <td>-0.212853</td>\n",
       "      <td>-0.565491</td>\n",
       "      <td>3.261837</td>\n",
       "      <td>-0.211146</td>\n",
       "      <td>-0.325670</td>\n",
       "      <td>3.224439</td>\n",
       "      <td>-0.200717</td>\n",
       "      <td>-0.084444</td>\n",
       "      <td>3.177295</td>\n",
       "      <td>-0.201521</td>\n",
       "      <td>0.034459</td>\n",
       "      <td>3.171535</td>\n",
       "      <td>-0.345760</td>\n",
       "      <td>-0.168635</td>\n",
       "      <td>3.246156</td>\n",
       "      <td>-0.352991</td>\n",
       "      <td>-0.413465</td>\n",
       "      <td>3.226118</td>\n",
       "      <td>-0.125305</td>\n",
       "      <td>-0.497324</td>\n",
       "      <td>3.254447</td>\n",
       "      <td>-0.074783</td>\n",
       "      <td>-0.511268</td>\n",
       "      <td>3.221581</td>\n",
       "      <td>-0.098649</td>\n",
       "      <td>-0.183750</td>\n",
       "      <td>3.140651</td>\n",
       "      <td>-0.120564</td>\n",
       "      <td>-0.426756</td>\n",
       "      <td>3.103806</td>\n",
       "      <td>-0.170770</td>\n",
       "      <td>-0.599483</td>\n",
       "      <td>3.057863</td>\n",
       "      <td>-0.182200</td>\n",
       "      <td>-0.649133</td>\n",
       "      <td>3.045605</td>\n",
       "      <td>-0.254438</td>\n",
       "      <td>-0.551961</td>\n",
       "      <td>3.250821</td>\n",
       "      <td>-0.295021</td>\n",
       "      <td>-0.860889</td>\n",
       "      <td>3.232168</td>\n",
       "      <td>-0.262815</td>\n",
       "      <td>-1.217263</td>\n",
       "      <td>3.338052</td>\n",
       "      <td>-0.289549</td>\n",
       "      <td>-1.246824</td>\n",
       "      <td>3.244711</td>\n",
       "      <td>-0.167236</td>\n",
       "      <td>-0.568644</td>\n",
       "      <td>3.214155</td>\n",
       "      <td>-0.208209</td>\n",
       "      <td>-0.892784</td>\n",
       "      <td>3.194621</td>\n",
       "      <td>-0.134968</td>\n",
       "      <td>-1.214255</td>\n",
       "      <td>3.245474</td>\n",
       "      <td>-0.164865</td>\n",
       "      <td>-1.262602</td>\n",
       "      <td>3.267416</td>\n",
       "      <td>-0.203971</td>\n",
       "      <td>-0.144745</td>\n",
       "      <td>3.190939</td>\n",
       "      <td>-0.064819</td>\n",
       "      <td>-0.560180</td>\n",
       "      <td>3.232672</td>\n",
       "      <td>-0.076017</td>\n",
       "      <td>-0.476868</td>\n",
       "      <td>3.200655</td>\n",
       "      <td>-0.203794</td>\n",
       "      <td>-0.706542</td>\n",
       "      <td>3.048331</td>\n",
       "      <td>-0.153736</td>\n",
       "      <td>-0.66522</td>\n",
       "      <td>3.102834</td>\n",
       "      <td>-0.214516</td>\n",
       "      <td>-0.559856</td>\n",
       "      <td>3.263403</td>\n",
       "      <td>-0.208001</td>\n",
       "      <td>-0.317874</td>\n",
       "      <td>3.228372</td>\n",
       "      <td>-0.195403</td>\n",
       "      <td>-0.076936</td>\n",
       "      <td>3.183481</td>\n",
       "      <td>-0.189596</td>\n",
       "      <td>0.048841</td>\n",
       "      <td>3.179868</td>\n",
       "      <td>-0.338588</td>\n",
       "      <td>-0.158595</td>\n",
       "      <td>3.251182</td>\n",
       "      <td>-0.345853</td>\n",
       "      <td>-0.403469</td>\n",
       "      <td>3.231146</td>\n",
       "      <td>-0.118191</td>\n",
       "      <td>-0.487359</td>\n",
       "      <td>3.259468</td>\n",
       "      <td>-0.068680</td>\n",
       "      <td>-0.492230</td>\n",
       "      <td>3.221683</td>\n",
       "      <td>-0.088417</td>\n",
       "      <td>-0.176614</td>\n",
       "      <td>3.145829</td>\n",
       "      <td>-0.118801</td>\n",
       "      <td>-0.418657</td>\n",
       "      <td>3.108189</td>\n",
       "      <td>-0.192521</td>\n",
       "      <td>-0.572072</td>\n",
       "      <td>3.040287</td>\n",
       "      <td>-0.214182</td>\n",
       "      <td>-0.613212</td>\n",
       "      <td>2.996442</td>\n",
       "      <td>-0.257144</td>\n",
       "      <td>-0.549525</td>\n",
       "      <td>3.252231</td>\n",
       "      <td>-0.284481</td>\n",
       "      <td>-0.867219</td>\n",
       "      <td>3.254666</td>\n",
       "      <td>-0.260625</td>\n",
       "      <td>-1.214738</td>\n",
       "      <td>3.346629</td>\n",
       "      <td>-0.287114</td>\n",
       "      <td>-1.243873</td>\n",
       "      <td>3.252890</td>\n",
       "      <td>-0.167253</td>\n",
       "      <td>-0.557715</td>\n",
       "      <td>3.216227</td>\n",
       "      <td>-0.177271</td>\n",
       "      <td>-0.884009</td>\n",
       "      <td>3.205395</td>\n",
       "      <td>-0.126542</td>\n",
       "      <td>-1.207360</td>\n",
       "      <td>3.246551</td>\n",
       "      <td>-0.155615</td>\n",
       "      <td>-1.256368</td>\n",
       "      <td>3.249898</td>\n",
       "      <td>-0.198894</td>\n",
       "      <td>-0.136859</td>\n",
       "      <td>3.196666</td>\n",
       "      <td>-0.064467</td>\n",
       "      <td>-0.539597</td>\n",
       "      <td>3.214681</td>\n",
       "      <td>-0.066076</td>\n",
       "      <td>-0.459657</td>\n",
       "      <td>3.206000</td>\n",
       "      <td>-0.227804</td>\n",
       "      <td>-0.654428</td>\n",
       "      <td>3.002556</td>\n",
       "      <td>-0.231687</td>\n",
       "      <td>-0.655208</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 3376 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.526048 -0.277147  2.987706 -0.606184 -0.010056  3.010000 -0.681454   \n",
       "1 -0.325320 -0.560200  3.244968 -0.296753 -0.312155  3.234485 -0.267607   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  0.251614  3.020046 -0.708330  0.386629  3.087164 -0.704835  0.111742   \n",
       "1 -0.066345  3.212031 -0.286594  0.049026  3.222926 -0.412259 -0.156479   \n",
       "\n",
       "         14        15        16        17        18        19        20  \\\n",
       "0  2.856311 -0.515683 -0.024701  2.727949 -0.353615 -0.123308  2.706112   \n",
       "1  3.294798 -0.467307 -0.373163  3.353605 -0.452381 -0.607216  3.276438   \n",
       "\n",
       "         21        22        23        24       25        26        27  \\\n",
       "0 -0.327895 -0.141611  2.749221 -0.511823  0.15791  3.311851 -0.543163   \n",
       "1 -0.459716 -0.668793  3.282482 -0.160399 -0.17987  3.150078 -0.136725   \n",
       "\n",
       "         28        29        30        31        32        33        34  \\\n",
       "0 -0.013495  3.108863 -0.496480 -0.181723  2.912678 -0.424105 -0.341968   \n",
       "1 -0.403121  3.128864 -0.160573 -0.601392  3.074458 -0.177965 -0.641335   \n",
       "\n",
       "         35        36        37        38        39        40        41  \\\n",
       "0  2.685757 -0.559787 -0.277896  2.867135 -0.187794 -0.440893  2.817494   \n",
       "1  3.074898 -0.368195 -0.549674  3.236222 -0.341016 -0.866046  3.282849   \n",
       "\n",
       "         42        43        44        45        46        47        48  \\\n",
       "0 -0.051320 -0.741197  2.696164  0.035544 -0.772924  2.653029 -0.479626   \n",
       "1 -0.313741 -1.186290  3.351244 -0.348155 -1.212369  3.259632 -0.276780   \n",
       "\n",
       "         49        50        51        52        53        54        55  \\\n",
       "0 -0.266888  3.033314 -0.132970 -0.302009  2.766374 -0.026497 -0.672848   \n",
       "1 -0.560417  3.195294 -0.206566 -0.869908  3.187913 -0.176944 -1.191492   \n",
       "\n",
       "         56        57        58        59        60        61        62  \\\n",
       "0  3.018918  0.066964 -0.706324  2.934041 -0.663448  0.187231  3.019821   \n",
       "1  3.236771 -0.216591 -1.252620  3.239565 -0.275193 -0.127509  3.219691   \n",
       "\n",
       "         63        64        65        66        67        68        69  \\\n",
       "0 -0.287726 -0.173169  2.772417 -0.374481 -0.201329  2.647500 -0.435660   \n",
       "1 -0.432343 -0.703650  3.251001 -0.433775 -0.662048  3.229998 -0.182828   \n",
       "\n",
       "         70        71        72        73        74        75        76  \\\n",
       "0 -0.399829  2.653183 -0.382046 -0.341303  2.709815 -0.522987 -0.302266   \n",
       "1 -0.695215  3.070847 -0.201468 -0.627444  3.066871 -0.325523 -0.560813   \n",
       "\n",
       "         77        78        79        80        81        82        83  \\\n",
       "0  2.985698 -0.606354 -0.018129  3.009436 -0.682353  0.250679  3.019942   \n",
       "1  3.245201 -0.297902 -0.313838  3.234329 -0.270614 -0.070106  3.211387   \n",
       "\n",
       "         84        85        86        87        88        89        90  \\\n",
       "0 -0.708845  0.385737  3.086951 -0.703320  0.110657  2.854790 -0.514419   \n",
       "1 -0.287911  0.048311  3.222532 -0.412941 -0.157385  3.293802 -0.467792   \n",
       "\n",
       "         91        92        93        94       95        96        97  \\\n",
       "0 -0.026362  2.726717 -0.351712 -0.124065  2.70648 -0.325711 -0.143484   \n",
       "1 -0.373858  3.352457 -0.452864 -0.607909  3.27529 -0.463337 -0.670728   \n",
       "\n",
       "         98        99       100       101       102       103       104  \\\n",
       "0  2.749665 -0.512720  0.156979  3.311748 -0.544056 -0.014422  3.108759   \n",
       "1  3.287665 -0.160529 -0.180116  3.150119 -0.136941 -0.403283  3.128651   \n",
       "\n",
       "        105       106       107       108       109       110       111  \\\n",
       "0 -0.496896 -0.182398  2.912465 -0.422955 -0.339052  2.685333 -0.555938   \n",
       "1 -0.160401 -0.601312  3.074679 -0.175291 -0.641988  3.075238 -0.368410   \n",
       "\n",
       "        112       113       114       115       116       117       118  \\\n",
       "0 -0.311383  2.888311 -0.187798 -0.440844  2.817507 -0.051158 -0.741131   \n",
       "1 -0.550553  3.236241 -0.341223 -0.866905  3.282910 -0.313706 -1.186323   \n",
       "\n",
       "        119       120       121       122       123       124       125  \\\n",
       "0  2.696049  0.036761 -0.770821  2.654296 -0.475196 -0.299777  3.005829   \n",
       "1  3.351247 -0.347867 -1.212483  3.259542 -0.276990 -0.560928  3.195813   \n",
       "\n",
       "        126       127       128       129       130       131       132  \\\n",
       "0 -0.132759 -0.301510  2.766153 -0.026572 -0.672512  3.018797  0.067243   \n",
       "1 -0.207239 -0.869591  3.188367 -0.177693 -1.190907  3.236062 -0.212907   \n",
       "\n",
       "        133       134       135       136       137       138       139  \\\n",
       "0 -0.705909  2.934533 -0.664326  0.185232  3.019666 -0.286222 -0.174110   \n",
       "1 -1.216706  3.144409 -0.277578 -0.130620  3.219173 -0.424656 -0.707095   \n",
       "\n",
       "        140       141       142       143       144       145       146  \\\n",
       "0  2.770132 -0.344511 -0.206511  2.714750 -0.432879 -0.389477  2.647004   \n",
       "1  3.226497 -0.437192 -0.655549  3.231333 -0.179751 -0.693261  3.062350   \n",
       "\n",
       "        147       148       149       150       151       152       153  \\\n",
       "0 -0.383055 -0.342364  2.709956 -0.522926 -0.291120  2.983348 -0.606446   \n",
       "1 -0.202178 -0.627065  3.072383 -0.325853 -0.560916  3.244719 -0.298965   \n",
       "\n",
       "        154       155       156       157       158       159       160  \\\n",
       "0 -0.017055  3.008816 -0.682771  0.250588  3.019696 -0.709493  0.385637   \n",
       "1 -0.314036  3.233568 -0.272403 -0.070329  3.210728 -0.289920  0.048205   \n",
       "\n",
       "        161       162       163       164       165       166       167  \\\n",
       "0  3.086540 -0.703649  0.110274  2.853598 -0.515603 -0.027185  2.724664   \n",
       "1  3.221743 -0.413437 -0.157693  3.293599 -0.478337 -0.376818  3.359510   \n",
       "\n",
       "        168       169       170       171       172       173       174  \\\n",
       "0 -0.352023 -0.123337  2.722028 -0.318310 -0.146859  2.754510 -0.513136   \n",
       "1 -0.463384 -0.610860  3.282351 -0.464546 -0.669435  3.289201 -0.161764   \n",
       "\n",
       "        175       176       177       178       179       180       181  \\\n",
       "0  0.156888  3.311502 -0.544470 -0.014513  3.108513 -0.497013 -0.182307   \n",
       "1 -0.179915  3.149890 -0.136978 -0.403274  3.128533 -0.160413 -0.601456   \n",
       "\n",
       "        182       183       184       185       186       187       188  \\\n",
       "0  2.912128 -0.423729 -0.339591  2.684088 -0.557001 -0.279047  2.854529   \n",
       "1  3.075129 -0.176269 -0.643616  3.076296 -0.368790 -0.550665  3.235907   \n",
       "\n",
       "        189       190       191       192       193       194       195  \\\n",
       "0 -0.187803 -0.440799  2.817520 -0.050492 -0.740845  2.695943  0.036925   \n",
       "1 -0.341253 -0.866930  3.282814 -0.313633 -1.185637  3.351261 -0.347889   \n",
       "\n",
       "        196       197      198       199       200       201       202  \\\n",
       "0 -0.771311  2.652386 -0.47548 -0.278758  3.026471 -0.131031 -0.301038   \n",
       "1 -1.211834  3.259632 -0.27723 -0.561029  3.195195 -0.207714 -0.869632   \n",
       "\n",
       "        203       204       205       206       207       208       209  \\\n",
       "0  2.763053 -0.026342 -0.671314  3.017473  0.067367 -0.704880  2.933144   \n",
       "1  3.188687 -0.177563 -1.190137  3.236064 -0.212873 -1.215974  3.144487   \n",
       "\n",
       "        210       211       212       213       214       215       216  \\\n",
       "0 -0.664581  0.185240  3.019431 -0.238680 -0.199818  2.774265 -0.309908   \n",
       "1 -0.279163 -0.130848  3.218463 -0.426399 -0.709989  3.239608 -0.441820   \n",
       "\n",
       "        217       218       219       220       221       222       223  \\\n",
       "0 -0.195522  2.768333 -0.438753 -0.393265  2.647284 -0.381671 -0.341660   \n",
       "1 -0.654967  3.240714 -0.180800 -0.694645  3.067773 -0.201585 -0.628618   \n",
       "\n",
       "        224       225       226       227       228       229       230  \\\n",
       "0  2.709762 -0.521120 -0.298138  2.983961 -0.606013 -0.017783  3.008922   \n",
       "1  3.069148 -0.326308 -0.561649  3.245134 -0.300993 -0.314916  3.233880   \n",
       "\n",
       "        231       232       233       234       235       236       237  \\\n",
       "0 -0.682892  0.251654  3.019938 -0.711226  0.386574  3.086356 -0.704142   \n",
       "1 -0.276975 -0.071172  3.211081 -0.290219  0.048108  3.221657 -0.417483   \n",
       "\n",
       "        238       239       240       241       242       243       244  \\\n",
       "0  0.110154  2.853352 -0.524689 -0.034344  2.719633 -0.305559 -0.157328   \n",
       "1 -0.161675  3.288854 -0.485017 -0.381054  3.357891 -0.470068 -0.615097   \n",
       "\n",
       "        245       246       247       248       249  ...      3126      3127  \\\n",
       "0  2.758009 -0.257899 -0.223257  2.782157 -0.513257  ... -0.117457 -0.297415   \n",
       "1  3.280774 -0.470077 -0.670854  3.297618 -0.163560  ... -0.251845 -0.931483   \n",
       "\n",
       "       3128      3129      3130      3131      3132      3133      3134  \\\n",
       "0  2.760189 -0.025414 -0.669564  3.016903  0.064981 -0.702720  2.926161   \n",
       "1  3.169451 -0.141850 -1.223032  3.245492 -0.170539 -1.264311  3.268951   \n",
       "\n",
       "       3135      3136      3137      3138      3139      3140      3141  \\\n",
       "0 -0.737668  0.232860  2.969651 -0.700851  0.599396  2.785406 -0.697228   \n",
       "1 -0.234057 -0.192967  3.174224 -0.066441 -0.590676  3.226854 -0.089689   \n",
       "\n",
       "       3142     3143      3144      3145      3146      3147      3148  \\\n",
       "0  0.531868  2.74700 -0.642330  0.376903  3.053223 -0.643093  0.381854   \n",
       "1 -0.546069  3.17625 -0.140819 -0.775892  3.070159 -0.130778 -0.709767   \n",
       "\n",
       "       3149      3150      3151      3152      3153      3154      3155  \\\n",
       "0  3.045156 -0.536747 -0.268661  3.019472 -0.651120  0.016701  2.996562   \n",
       "1  3.030000 -0.244266 -0.595101  3.253821 -0.224001 -0.341765  3.220140   \n",
       "\n",
       "       3156      3157      3158      3159      3160      3161      3162  \\\n",
       "0 -0.765691  0.305620  2.962245 -0.793885  0.438116  3.046815 -0.760563   \n",
       "1 -0.203489 -0.093072  3.174690 -0.211584  0.024953  3.165083 -0.350594   \n",
       "\n",
       "       3163      3164      3165      3166      3167      3168      3169  \\\n",
       "0  0.104213  2.855314 -0.655104  0.219293  2.729084 -0.641481  0.472357   \n",
       "1 -0.174678  3.246146 -0.357803 -0.419481  3.226108 -0.130104 -0.503322   \n",
       "\n",
       "       3170      3171      3172      3173      3174      3175      3176  \\\n",
       "0  2.719059 -0.662671  0.529586  2.742141 -0.595748  0.211718  3.254298   \n",
       "1  3.254441 -0.076425 -0.515020  3.219594 -0.119309 -0.210774  3.128807   \n",
       "\n",
       "       3177      3178      3179      3180      3181      3182      3183  \\\n",
       "0 -0.651054  0.271530  3.402166 -0.646946  0.347994  3.149582 -0.646372   \n",
       "1 -0.118202 -0.455402  3.102370 -0.164457 -0.648000  3.061667 -0.165752   \n",
       "\n",
       "       3184      3185      3186      3187      3188      3189      3190  \\\n",
       "0  0.373832  3.069490 -0.575384 -0.279854  2.961231 -0.254864 -0.200216   \n",
       "1 -0.683368  3.049949 -0.289343 -0.585349  3.245842 -0.296638 -0.865806   \n",
       "\n",
       "       3191      3192      3193      3194      3195      3196      3197  \\\n",
       "0  2.798510 -0.091002 -0.637781  2.719589  0.000928 -0.670706  2.630121   \n",
       "1  3.206981 -0.262789 -1.214496  3.333538 -0.215802 -1.270810  3.309790   \n",
       "\n",
       "       3198      3199      3200      3201      3202      3203      3204  \\\n",
       "0 -0.484576 -0.250839  3.007088 -0.141068 -0.346020  2.762192 -0.024301   \n",
       "1 -0.194448 -0.593716  3.203144 -0.230097 -0.918363  3.179335 -0.140089   \n",
       "\n",
       "       3205      3206      3207      3208      3209      3210      3211  \\\n",
       "0 -0.699762  3.033093  0.066705 -0.732814  2.943577 -0.737606  0.233676   \n",
       "1 -1.220089  3.243972 -0.167436 -1.262573  3.268584 -0.208444 -0.154559   \n",
       "\n",
       "       3212      3213      3214      3215      3216      3217      3218  \\\n",
       "0  2.972858 -0.707763  0.589335  2.780041 -0.678035  0.497967  2.730250   \n",
       "1  3.188059 -0.064566 -0.562389  3.220170 -0.076752 -0.477937  3.203286   \n",
       "\n",
       "       3219      3220      3221      3222      3223      3224      3225  \\\n",
       "0 -0.642994  0.380211  3.053722 -0.643756  0.385148  3.045661 -0.536393   \n",
       "1 -0.179791 -0.734700  3.057266 -0.145970 -0.691034  3.035231 -0.212853   \n",
       "\n",
       "       3226      3227      3228      3229      3230      3231      3232  \\\n",
       "0 -0.269593  3.019098 -0.650550  0.015861  2.994569 -0.761782  0.300374   \n",
       "1 -0.565491  3.261837 -0.211146 -0.325670  3.224439 -0.200717 -0.084444   \n",
       "\n",
       "       3233      3234      3235      3236      3237      3238      3239  \\\n",
       "0  2.954222 -0.794149  0.430042  3.045948 -0.760232  0.105684  2.853519   \n",
       "1  3.177295 -0.201521  0.034459  3.171535 -0.345760 -0.168635  3.246156   \n",
       "\n",
       "       3240      3241      3242      3243      3244      3245      3246  \\\n",
       "0 -0.640828  0.247795  2.727601 -0.636687  0.500929  2.718496 -0.640456   \n",
       "1 -0.352991 -0.413465  3.226118 -0.125305 -0.497324  3.254447 -0.074783   \n",
       "\n",
       "       3247      3248      3249      3250      3251      3252      3253  \\\n",
       "0  0.596024  2.740083 -0.591833  0.206482  3.246271 -0.682753  0.257004   \n",
       "1 -0.511268  3.221581 -0.098649 -0.183750  3.140651 -0.120564 -0.426756   \n",
       "\n",
       "       3254      3255      3256      3257      3258      3259      3260  \\\n",
       "0  3.380537 -0.662777  0.361562  3.139406 -0.647476  0.609547  2.747746   \n",
       "1  3.103806 -0.170770 -0.599483  3.057863 -0.182200 -0.649133  3.045605   \n",
       "\n",
       "       3261      3262      3263      3264      3265      3266      3267  \\\n",
       "0 -0.575439 -0.280800  2.960728 -0.254915 -0.199929  2.798599 -0.091137   \n",
       "1 -0.254438 -0.551961  3.250821 -0.295021 -0.860889  3.232168 -0.262815   \n",
       "\n",
       "       3268      3269      3270      3271      3272      3273      3274  \\\n",
       "0 -0.637441  2.719661  0.000637 -0.670160  2.629707 -0.483578 -0.252083   \n",
       "1 -1.217263  3.338052 -0.289549 -1.246824  3.244711 -0.167236 -0.568644   \n",
       "\n",
       "       3275      3276      3277      3278      3279      3280      3281  \\\n",
       "0  3.005907 -0.142665 -0.296575  2.762277 -0.031544 -0.666042  3.015459   \n",
       "1  3.214155 -0.208209 -0.892784  3.194621 -0.134968 -1.214255  3.245474   \n",
       "\n",
       "       3282      3283      3284      3285      3286      3287      3288  \\\n",
       "0  0.059394 -0.699224  2.925381 -0.735025  0.230247  2.967056 -0.647058   \n",
       "1 -0.164865 -1.262602  3.267416 -0.203971 -0.144745  3.190939 -0.064819   \n",
       "\n",
       "       3289      3290      3291      3292      3293      3294      3295  \\\n",
       "0  0.645758  2.747567 -0.669871  0.607714  2.749000 -0.631756  0.626769   \n",
       "1 -0.560180  3.232672 -0.076017 -0.476868  3.200655 -0.203794 -0.706542   \n",
       "\n",
       "       3296      3297     3298      3299      3300      3301      3302  \\\n",
       "0  2.731108 -0.653891  0.64260  2.740333 -0.535844 -0.287107  3.009541   \n",
       "1  3.048331 -0.153736 -0.66522  3.102834 -0.214516 -0.559856  3.263403   \n",
       "\n",
       "       3303      3304      3305      3306      3307      3308      3309  \\\n",
       "0 -0.641248 -0.016169  2.989267 -0.737427  0.244177  2.959609 -0.789884   \n",
       "1 -0.208001 -0.317874  3.228372 -0.195403 -0.076936  3.183481 -0.189596   \n",
       "\n",
       "       3310      3311      3312      3313      3314      3315      3316  \\\n",
       "0  0.407848  3.048982 -0.729282  0.171244  2.829513 -0.625358  0.308465   \n",
       "1  0.048841  3.179868 -0.338588 -0.158595  3.251182 -0.345853 -0.403469   \n",
       "\n",
       "       3317      3318      3319      3320      3321      3322      3323  \\\n",
       "0  2.731224 -0.586710  0.542867  2.691673 -0.596145  0.636903  2.709147   \n",
       "1  3.231146 -0.118191 -0.487359  3.259468 -0.068680 -0.492230  3.221683   \n",
       "\n",
       "       3324      3325      3326      3327      3328      3329      3330  \\\n",
       "0 -0.567538  0.150466  3.251560 -0.574421  0.303848  3.075653 -0.596376   \n",
       "1 -0.088417 -0.176614  3.145829 -0.118801 -0.418657  3.108189 -0.192521   \n",
       "\n",
       "       3331      3332      3333      3334      3335      3336      3337  \\\n",
       "0  0.471110  2.875680 -0.589655  0.586514  2.717145 -0.570154 -0.290760   \n",
       "1 -0.572072  3.040287 -0.214182 -0.613212  2.996442 -0.257144 -0.549525   \n",
       "\n",
       "       3338      3339      3340      3341      3342      3343      3344  \\\n",
       "0  2.955790 -0.255583 -0.187767  2.804032 -0.094777 -0.625936  2.721997   \n",
       "1  3.252231 -0.284481 -0.867219  3.254666 -0.260625 -1.214738  3.346629   \n",
       "\n",
       "       3345      3346      3347      3348      3349      3350      3351  \\\n",
       "0  0.002962 -0.660430  2.645224 -0.491591 -0.281821  2.985380 -0.109749   \n",
       "1 -0.287114 -1.243873  3.252890 -0.167253 -0.557715  3.216227 -0.177271   \n",
       "\n",
       "       3352      3353      3354      3355      3356      3357      3358  \\\n",
       "0 -0.353734  2.758277 -0.016498 -0.707223  3.037502  0.080172 -0.741614   \n",
       "1 -0.884009  3.205395 -0.126542 -1.207360  3.246551 -0.155615 -1.256368   \n",
       "\n",
       "       3359      3360      3361      3362      3363      3364      3365  \\\n",
       "0  2.960732 -0.713666  0.178025  2.969061 -0.599133  0.702776  2.722800   \n",
       "1  3.249898 -0.198894 -0.136859  3.196666 -0.064467 -0.539597  3.214681   \n",
       "\n",
       "       3366      3367      3368      3369      3370      3371      3372  \\\n",
       "0 -0.619659  0.647852  2.708625 -0.598600  0.593635  2.707864 -0.601837   \n",
       "1 -0.066076 -0.459657  3.206000 -0.227804 -0.654428  3.002556 -0.231687   \n",
       "\n",
       "       3373      3374  labels  \n",
       "0  0.592191  2.697301       0  \n",
       "1 -0.655208  3.000000       1  \n",
       "\n",
       "[2 rows x 3376 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skeletons.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax norm\n",
    "def normalization_minmax(data):\n",
    "    data.iloc[:,:-1] = data.iloc[:,:-1].apply((lambda x: (x - x.min()) / (x.max() - x.min())), axis=0)\n",
    "\n",
    "    return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a tensor image with mean and standard deviation\n",
    "def normalization(data):\n",
    "    data.iloc[:,:-1] = data.iloc[:,:-1].apply((lambda x: (x - x.mean()) / x.std()), axis=0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return LABELS[category_i], category_i\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skeleton_Dataset(Dataset):\n",
    "    def __init__(self, file_path,  transform=None):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        if transform is not None:\n",
    "            self.transform=transform(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.transform is not None:\n",
    "            return len(self.transform)\n",
    "        else:\n",
    "            return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is not None:\n",
    "            item = np.asarray(self.transform.iloc[idx,:-1]).reshape(45,75)\n",
    "        else:\n",
    "            item = np.asarray(self.data.iloc[idx,:-1]).reshape(45,75)\n",
    "            \n",
    "        label = self.data.iloc[idx,-1]\n",
    "\n",
    "        return (item, label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Skeleton_Dataset(file_path = \"skels.csv\", transform=normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.75*len(dataset)),int(0.25*len(dataset))])\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,layer_num):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim,layer_num,batch_first=True)\n",
    "#         self.dr = torch.nn.Dropout2d(0.25)\n",
    "        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = inputs\n",
    "        lstm_out,(hn,cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_net(\n",
       "  (lstm): LSTM(75, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 128\n",
    "n_joints = 25*3\n",
    "n_categories = len(LABELS)\n",
    "n_layer = 2\n",
    "rnn = LSTM_net(n_joints,n_hidden,n_categories,n_layer)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0007\n",
    "optimizer = optim.SGD(rnn.parameters(),lr=learning_rate,momentum=0.9)\n",
    "epoches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 iter : 0 (0m 0s) 1.6263  / cheer up ✗ (jump up)\n",
      "epoch : 16 iter : 20 (0m 15s) 1.5653  / clapping ✗ (hand waving)\n",
      "epoch : 33 iter : 10 (0m 30s) 1.5122  / cheer up ✗ (jump up)\n",
      "epoch : 50 iter : 0 (0m 46s) 1.3180  / sitting down ✗ (jump up)\n",
      "epoch : 66 iter : 20 (1m 1s) 0.7087  / sitting down ✓\n",
      "epoch : 83 iter : 10 (1m 18s) 0.7774  / sitting down ✓\n"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for epoch in range(epoches):  \n",
    "    current_loss = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output = rnn(inputs.float())\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        category = LABELS[int(labels[0])]\n",
    "\n",
    "        if counter % 500 == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            #####сделать более аккуратный вывод для каждой эпохии\n",
    "            ######сделать обновление графика loss по ходу обучению  \n",
    "            print('epoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "    if counter % 100 == 0:\n",
    "        all_losses.append(current_loss / 25)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network:   80.0\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "right = 0\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        counter = counter + 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)   \n",
    "        output = rnn(inputs.float())\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        category = LABELS[int(labels[0])]\n",
    "        \n",
    "        if guess == category:\n",
    "            right = right + 1\n",
    "\n",
    "\n",
    "print('Accuracy of the network:  ',  (100 * right / counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результат\n",
    "epoches=300  \n",
    "Accuracy of the network:   70.0 - без нормализации  \n",
    "Accuracy of the network:   80.0 - norm mean_std  \n",
    "Accuracy of the network:   60.0 - norm min_max  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "a. Сгенерировать меньший датасет из 8-10 классов движения  \n",
    "b. Изменить функцию разбиения датасета на тестовую и тренировочную часть, увеличив кусок, отводимый на тест  \n",
    "c. Обучить уже существующую модель (предварительно проанализировав какие параметры нужно менять)  \n",
    "d. Изменить модель для улучшения качества  \n",
    "e. Сгенерировать другой датасет с меньшим количеством “кадром” в нашей серии и сравнить с улучшилось или ухудшилось качество предсказания. Провести несколько таких итераций, дать свою оценку уменьшению кадров, назвать оптимальное, на ваш взгляд, их количество.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выполнение:  \n",
    "    - скачан датасет из http://rose1.ntu.edu.sg/datasets/actionrecognition.asp в формате .SKELETON  \n",
    "    - датасет конвертирован https://github.com/shahroudy/NTURGB-D в .npy  \n",
    "    - выбраны 10 классов:  \n",
    "    (LABELS = {0: \"cheer up\", 1: \"jump up\", 2:  \"hand waving\", 3: \"sitting down\", 4: \"clapping\",  \n",
    "                    5:  \"tear up paper\", 6: \"put on glasses\", 7: \"phone call\", 8: \"take off jacket\", 9: \"wipe face\"}  \n",
    "    - датасет получился 9412x3376 весом ~400Mb (Код создания датасета вконце нотбука)\n",
    "    - Изменено разбиение датасета на 30% Train 70% Test\n",
    "    - Изменены параметры модели:\n",
    "            n_hidden = 64\n",
    "            n_layer = 3\n",
    "            lstm_dropout = 0.1\n",
    "            epoches = 100\n",
    "    - При уменьшении кадров вконце (оставались кадры с 0 и далее) качество сразу падало, При ученьшении кадров вначале (оставались с 16 по 45) качетво не страдало, а скарость обучения увеличевалась. Нормализация положительно на результат не влияла. Поэтому лучший вариант получился: оставить с 16 по 45 кадр, без нормализации.  (Замеры Accuracy находятся далее в нотбуке после модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skeleton_Dataset(Dataset):\n",
    "    def __init__(self, file_path, frames=45, transform=None, frame_reduce=None):\n",
    "        self.frames = frames\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.transform = transform\n",
    "        if frame_reduce is not None:\n",
    "            self.data = frame_reduce(self.data, self.frames)\n",
    "        if transform is not None:\n",
    "            self.transform = transform(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.transform is not None:\n",
    "            return len(self.transform)\n",
    "        else:\n",
    "            return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is not None:\n",
    "            item = np.asarray(self.transform.iloc[idx,:-1]).reshape(self.frames,75)\n",
    "        else:\n",
    "            item = np.asarray(self.data.iloc[idx,:-1]).reshape(self.frames,75)\n",
    "            \n",
    "        label = self.data.iloc[idx,-1]\n",
    "\n",
    "        return (item, label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_frame_amount(data, frames=45):\n",
    "    total_frames = 45\n",
    "    #data = data.drop(data.columns[-(75*(total_frames-frames)+1):-1], axis = 1) # Удаление вконце\n",
    "    data = data.drop(data.columns[:(75*(total_frames-frames))], axis = 1) # Удаление вначале\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {0: \"cheer up\", 1: \"jump up\", 2:  \"hand waving\", 3: \"sitting down\", 4: \"clapping\",\n",
    "                    5:  \"tear up paper\", 6: \"put on glasses\", 7: \"phone call\", 8: \"take off jacket\", 9: \"wipe face\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeletons_10 = pd.read_csv(\"skels_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9412, 3376)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skeletons_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeletons_10.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeletons_10.to_csv('skels_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_10 = Skeleton_Dataset(file_path = \"skels_10.csv\", frames=45, transform=None, frame_reduce=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part = int(0.30*len(dataset_10))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset_10, [train_part, int(len(dataset_10) - train_part)])\n",
    "train_loader = DataLoader(train_dataset, batch_size = 16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, layer_num, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, layer_num, dropout=dropout, batch_first=True)\n",
    "#        self.dr = torch.nn.Dropout2d(0.25)\n",
    "        self.fc = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = inputs\n",
    "        lstm_out,(hn,cn) = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_net(\n",
       "  (lstm): LSTM(75, 64, num_layers=3, batch_first=True, dropout=0.1)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 64\n",
    "n_joints = 25*3\n",
    "n_categories = len(LABELS)\n",
    "n_layer = 3\n",
    "lstm_dropout = 0.1\n",
    "rnn = LSTM_net(n_joints,n_hidden,n_categories,n_layer, lstm_dropout)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0007\n",
    "optimizer = optim.Adam(rnn.parameters(),lr=learning_rate)\n",
    "epoches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 iter : 0 (0m 0s) 2.3375  / take off jacket ✗ (jump up)\n",
      "epoch : 2 iter : 146 (0m 19s) 1.8025  / wipe face ✗ (hand waving)\n",
      "epoch : 5 iter : 115 (0m 40s) 1.1548  / phone call ✗ (wipe face)\n",
      "epoch : 8 iter : 84 (1m 0s) 1.7280  / take off jacket ✗ (cheer up)\n",
      "epoch : 11 iter : 53 (1m 20s) 1.5409  / hand waving ✓\n",
      "epoch : 14 iter : 22 (1m 41s) 1.3158  / take off jacket ✓\n",
      "epoch : 16 iter : 168 (2m 1s) 0.7233  / cheer up ✓\n",
      "epoch : 19 iter : 137 (2m 22s) 1.4724  / put on glasses ✗ (take off jacket)\n",
      "epoch : 22 iter : 106 (2m 43s) 0.9621  / take off jacket ✓\n",
      "epoch : 25 iter : 75 (3m 3s) 1.3915  / wipe face ✓\n",
      "epoch : 28 iter : 44 (3m 24s) 0.7623  / hand waving ✓\n",
      "epoch : 31 iter : 13 (3m 44s) 0.6512  / cheer up ✓\n",
      "epoch : 33 iter : 159 (4m 4s) 1.3512  / clapping ✓\n",
      "epoch : 36 iter : 128 (4m 25s) 0.7373  / put on glasses ✓\n",
      "epoch : 39 iter : 97 (4m 45s) 0.5403  / put on glasses ✓\n",
      "epoch : 42 iter : 66 (5m 6s) 0.8329  / take off jacket ✓\n",
      "epoch : 45 iter : 35 (5m 26s) 0.9425  / phone call ✓\n",
      "epoch : 48 iter : 4 (5m 46s) 0.5351  / put on glasses ✓\n",
      "epoch : 50 iter : 150 (6m 6s) 0.8736  / phone call ✗ (wipe face)\n",
      "epoch : 53 iter : 119 (6m 26s) 0.4643  / cheer up ✓\n",
      "epoch : 56 iter : 88 (6m 46s) 0.5134  / wipe face ✗ (clapping)\n",
      "epoch : 59 iter : 57 (7m 7s) 0.6475  / hand waving ✓\n",
      "epoch : 62 iter : 26 (7m 27s) 0.5482  / sitting down ✓\n",
      "epoch : 64 iter : 172 (7m 47s) 0.6353  / phone call ✓\n",
      "epoch : 67 iter : 141 (8m 7s) 0.3853  / put on glasses ✓\n",
      "epoch : 70 iter : 110 (8m 27s) 0.2597  / take off jacket ✓\n",
      "epoch : 73 iter : 79 (8m 48s) 0.3425  / take off jacket ✗ (cheer up)\n",
      "epoch : 76 iter : 48 (9m 8s) 0.1701  / sitting down ✓\n",
      "epoch : 79 iter : 17 (9m 28s) 0.0779  / put on glasses ✓\n",
      "epoch : 81 iter : 163 (9m 49s) 0.1927  / phone call ✓\n",
      "epoch : 84 iter : 132 (10m 10s) 0.4043  / sitting down ✓\n",
      "epoch : 87 iter : 101 (10m 30s) 0.2977  / cheer up ✗ (clapping)\n",
      "epoch : 90 iter : 70 (10m 50s) 0.1731  / phone call ✓\n",
      "epoch : 93 iter : 39 (11m 9s) 0.1347  / put on glasses ✓\n",
      "epoch : 96 iter : 8 (11m 29s) 0.6580  / sitting down ✓\n",
      "epoch : 98 iter : 154 (11m 50s) 0.4629  / clapping ✓\n"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for epoch in range(epoches):  \n",
    "    current_loss = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        output = rnn(inputs.float())\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        category = LABELS[int(labels[0])]\n",
    "\n",
    "        if counter % 500 == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            #####сделать более аккуратный вывод для каждой эпохии\n",
    "            ######сделать обновление графика loss по ходу обучению  \n",
    "            print('epoch : %d iter : %d (%s) %.4f  / %s %s' % (epoch, i, timeSince(start), loss, guess, correct))\n",
    "\n",
    "        \n",
    "        counter = counter + 1\n",
    "    if counter % 100 == 0:\n",
    "        all_losses.append(current_loss / 25)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network:   67.47572815533981\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "right = 0\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        counter = counter + 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)   \n",
    "        output = rnn(inputs.float())\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        category = LABELS[int(labels[0])]\n",
    "        \n",
    "        if guess == category:\n",
    "            right = right + 1\n",
    "\n",
    "\n",
    "print('Accuracy of the network:  ',  (100 * right / counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замеры\n",
    "\n",
    "n_hidden = 64  \n",
    "n_joints = 25*3  \n",
    "n_categories = len(LABELS)  \n",
    "n_layer = 3  \n",
    "lstm_dropout = 0.1  \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "learning_rate = 0.0007  \n",
    "optimizer = optim.Adam(rnn.parameters(),lr=learning_rate)  \n",
    "epoches = 100  \n",
    "\n",
    "Все кадры  \n",
    "<15 min  \n",
    "45 frames -> Accuracy of the network:   70.63106796116504\n",
    "\n",
    "Удаление кадров вконце  \n",
    "<10 min  \n",
    "30 frames -> Accuracy of the network:   59.70873786407767  \n",
    "<7 min  \n",
    "15 frames -> Accuracy of the network:   27.42718446601942  \n",
    "    \n",
    "Удаление кадров вначале   \n",
    "<10 min  \n",
    "30 frames ->  Accuracy of the network:   70.87378640776699  \n",
    "<7 min  \n",
    "15 frames -> Accuracy of the network:   58.737864077669904  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nframes_limit = 45\\nlabels_codes = {\\n'A022': 0, 'A027':1, 'A023':2, 'A008':3, 'A010':4,\\n'A013': 5, 'A018':6, 'A028':7, 'A015':8, 'A037':9,\\n}\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "frames_limit = 45\n",
    "labels_codes = {\n",
    "'A022': 0, 'A027':1, 'A023':2, 'A008':3, 'A010':4,\n",
    "'A013': 5, 'A018':6, 'A028':7, 'A015':8, 'A037':9,\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\n\\nimport glob\\n\\nskels_10 = pd.DataFrame()\\n\\nfor np_name in glob.glob('../data/raw_npy/*.np[yz]'):\\n    if np_name[-17:-13] in labels_codes.keys():\\n        x = np.load(np_name, allow_pickle=True).item()\\n        frames = x['skel_body0'].shape[0]\\n        label = x['file_name'][-4:]\\n        x = x['skel_body0'].reshape(frames, 75)[:frames_limit].flatten()\\n        x = (pd.DataFrame(x)).T\\n        x['labels'] = labels_codes[label]\\n        if skels_10.empty:\\n            skels_10 = x\\n        else:\\n            skels_10 = skels_10.append(x)\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "\n",
    "import glob\n",
    "\n",
    "skels_10 = pd.DataFrame()\n",
    "\n",
    "for np_name in glob.glob('../data/raw_npy/*.np[yz]'):\n",
    "    if np_name[-17:-13] in labels_codes.keys():\n",
    "        x = np.load(np_name, allow_pickle=True).item()\n",
    "        frames = x['skel_body0'].shape[0]\n",
    "        label = x['file_name'][-4:]\n",
    "        x = x['skel_body0'].reshape(frames, 75)[:frames_limit].flatten()\n",
    "        x = (pd.DataFrame(x)).T\n",
    "        x['labels'] = labels_codes[label]\n",
    "        if skels_10.empty:\n",
    "            skels_10 = x\n",
    "        else:\n",
    "            skels_10 = skels_10.append(x)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skels_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skels_10.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skels_10.to_csv('skels_10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_keys = {\n",
    "'A001': 'drink water', 'A002': 'eat meal', 'A003': 'brush teeth', 'A004': 'brush hair',\n",
    "'A005': 'drop', 'A006': 'pick up', 'A007': 'throw', 'A008': 'sit down',\n",
    "'A009': 'stand up', 'A010': 'clapping', 'A011': 'reading', 'A012': 'writing',\n",
    "'A013': 'tear up paper', 'A014': 'put on jacket', 'A015': 'take off jacket', 'A016': 'put on a shoe',\n",
    "'A017': 'take off a shoe', 'A018': 'put on glasses', 'A019': 'take off glasses', 'A020': 'put on a hat/cap',\n",
    "'A021': 'take off a hat/cap', 'A022': 'cheer up', 'A023': 'hand waving', 'A024': 'kicking something',\n",
    "'A025': 'reach into pocket', 'A026': 'hopping', 'A027': 'jump up', 'A028': 'phone call',\n",
    "'A029': 'play with phone/tablet', 'A030': 'type on a keyboard', 'A031': 'point to something', 'A032': 'taking a selfie',\n",
    "'A033': 'check time (from watch)', 'A034': 'rub two hands', 'A035': 'nod head/bow', 'A036': 'shake head',\n",
    "'A037': 'wipe face', 'A038': 'salute', 'A039': 'put palms together', 'A040': 'cross hands in front',\n",
    "'A041': 'sneeze/cough', 'A042': 'staggering', 'A043': 'falling down', 'A044': 'headache',\n",
    "'A045': 'chest pain', 'A046': 'back pain', 'A047': 'neck pain', 'A048': 'nausea/vomiting', 'A049': 'fan self',\n",
    "'A050': 'punch/slap', 'A051': 'kicking', 'A052': 'pushing', 'A053': 'pat on back',\n",
    "'A054': 'point finger', 'A055': 'hugging', 'A056': 'giving object', 'A057': 'touch pocket',\n",
    "'A058': 'shaking hands', 'A059': 'walking towards', 'A060': 'walking apart'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_codes = {\n",
    "'A022': 0, 'A027':1, 'A023':2, 'A008':3, 'A010':4,\n",
    "'A013': 5, 'A018':6, 'A028':7, 'A015':8, 'A037':9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
